<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Airflow – Home</title>
    <link>/</link>
    <description>Recent content in Home on Apache Airflow</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
      
      
    
    
    <item>
      <title>Blog: Apache Airflow 2.0 is here!</title>
      <link>/blog/airflow-two-point-oh-is-here/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-two-point-oh-is-here/</guid>
      <description>
        
        
        

&lt;p&gt;I am proud to announce that Apache Airflow 2.0.0 has been released.&lt;/p&gt;

&lt;p&gt;The full changelog is about 3,000 lines long (already excluding everything backported to 1.10), so for now I&amp;rsquo;ll simply share some of the major features in 2.0.0 compared to 1.10.14:&lt;/p&gt;

&lt;h2 id=&#34;a-new-way-of-writing-dags-the-taskflow-api-aip-31&#34;&gt;A new way of writing dags: the TaskFlow API (AIP-31)&lt;/h2&gt;

&lt;p&gt;(Known in 2.0.0alphas as Functional DAGs.)&lt;/p&gt;

&lt;p&gt;DAGs are now much much nicer to author especially when using PythonOperator. Dependencies are handled more clearly and XCom is nicer to use&lt;/p&gt;

&lt;p&gt;Read more here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html&#34; target=&#34;_blank&#34;&gt;TaskFlow API Tutorial&lt;/a&gt; &lt;br /&gt;
&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#decorated-flows&#34; target=&#34;_blank&#34;&gt;TaskFlow API Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A quick teaser of what DAGs can now look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airflow.decorators import dag, task
from airflow.utils.dates import days_ago

@dag(default_args={&#39;owner&#39;: &#39;airflow&#39;}, schedule_interval=None, start_date=days_ago(2))
def tutorial_taskflow_api_etl():
   @task
   def extract():
       return {&amp;quot;1001&amp;quot;: 301.27, &amp;quot;1002&amp;quot;: 433.21, &amp;quot;1003&amp;quot;: 502.22}

   @task
   def transform(order_data_dict: dict) -&amp;gt; dict:
       total_order_value = 0

       for value in order_data_dict.values():
           total_order_value += value

       return {&amp;quot;total_order_value&amp;quot;: total_order_value}

   @task()
   def load(total_order_value: float):

       print(&amp;quot;Total order value is: %.2f&amp;quot; % total_order_value)

   order_data = extract()
   order_summary = transform(order_data)
   load(order_summary[&amp;quot;total_order_value&amp;quot;])

tutorial_etl_dag = tutorial_taskflow_api_etl()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fully-specified-rest-api-aip-32&#34;&gt;Fully specified REST API (AIP-32)&lt;/h2&gt;

&lt;p&gt;We now have a fully supported, no-longer-experimental API with a comprehensive OpenAPI specification&lt;/p&gt;

&lt;p&gt;Read more here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html&#34; target=&#34;_blank&#34;&gt;REST API Documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;massive-scheduler-performance-improvements&#34;&gt;Massive Scheduler performance improvements&lt;/h2&gt;

&lt;p&gt;As part of AIP-15 (Scheduler HA+performance) and other work Kamil did, we significantly improved the performance of the Airflow Scheduler. It now starts tasks much, MUCH quicker.&lt;/p&gt;

&lt;p&gt;Over at Astronomer.io we&amp;rsquo;ve &lt;a href=&#34;https://www.astronomer.io/blog/airflow-2-scheduler&#34; target=&#34;_blank&#34;&gt;benchmarked the scheduler—it&amp;rsquo;s fast&lt;/a&gt; (we had to triple check the numbers as we don&amp;rsquo;t quite believe them at first!)&lt;/p&gt;

&lt;h2 id=&#34;scheduler-is-now-ha-compatible-aip-15&#34;&gt;Scheduler is now HA compatible (AIP-15)&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s now possible and supported to run more than a single scheduler instance. This is super useful for both resiliency (in case a scheduler goes down) and scheduling performance.&lt;/p&gt;

&lt;p&gt;To fully use this feature you need Postgres 9.6+ or MySQL 8+ (MySQL 5, and MariaDB won&amp;rsquo;t work with more than one scheduler I&amp;rsquo;m afraid).&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no config or other set up required to run more than one scheduler—just start up a scheduler somewhere else (ensuring it has access to the DAG files) and it will cooperate with your existing schedulers through the database.&lt;/p&gt;

&lt;p&gt;For more information, read the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/scheduler.html#running-more-than-one-scheduler&#34; target=&#34;_blank&#34;&gt;Scheduler HA documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;task-groups-aip-34&#34;&gt;Task Groups (AIP-34)&lt;/h2&gt;

&lt;p&gt;SubDAGs were commonly used for grouping tasks in the UI, but they had many drawbacks in their execution behaviour (primarily that they only executed a single task in parallel!) To improve this experience, we’ve introduced &amp;ldquo;Task Groups&amp;rdquo;: a method for organizing tasks which provides the same grouping behaviour as a subdag without any of the execution-time drawbacks.&lt;/p&gt;

&lt;p&gt;SubDAGs will still work for now, but we think that any previous use of SubDAGs can now be replaced with task groups. If you find an example where this isn&amp;rsquo;t the case, please let us know by opening an issue on GitHub&lt;/p&gt;

&lt;p&gt;For more information, check out the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/concepts.html#taskgroup&#34; target=&#34;_blank&#34;&gt;Task Group documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;refreshed-ui&#34;&gt;Refreshed UI&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve given the Airflow UI &lt;a href=&#34;https://github.com/apache/airflow/pull/11195&#34; target=&#34;_blank&#34;&gt;a visual refresh&lt;/a&gt; and updated some of the styling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;airflow-2.0-ui.gif&#34; alt=&#34;Airflow 2.0&#39;s new UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We have also added an option to auto-refresh task states in Graph View so you no longer need to continuously press the refresh button :).&lt;/p&gt;

&lt;p&gt;Check out &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow/stable/ui.html&#34; target=&#34;_blank&#34;&gt;the screenshots in the docs&lt;/a&gt; for more.&lt;/p&gt;

&lt;h2 id=&#34;smart-sensors-for-reduced-load-from-sensors-aip-17&#34;&gt;Smart Sensors for reduced load from sensors (AIP-17)&lt;/h2&gt;

&lt;p&gt;If you make heavy use of sensors in your Airflow cluster, you might find that sensor execution takes up a significant proportion of your cluster even with &amp;ldquo;reschedule&amp;rdquo; mode. To improve this, we&amp;rsquo;ve added a new mode called &amp;ldquo;Smart Sensors&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;This feature is in &amp;ldquo;early-access&amp;rdquo;: it&amp;rsquo;s been well-tested by Airbnb and is &amp;ldquo;stable&amp;rdquo;/usable, but we reserve the right to make backwards incompatible changes to it in a future release (if we have to. We&amp;rsquo;ll try very hard not to!)&lt;/p&gt;

&lt;p&gt;Read more about it in the &lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/smart-sensor.html&#34; target=&#34;_blank&#34;&gt;Smart Sensors documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;simplified-kubernetesexecutor&#34;&gt;Simplified KubernetesExecutor&lt;/h2&gt;

&lt;p&gt;For Airflow 2.0, we have re-architected the KubernetesExecutor in a fashion that is simultaneously faster, easier to understand, and more flexible for Airflow users. Users will now be able to access the full Kubernetes API to create a .yaml &lt;code&gt;pod_template_file&lt;/code&gt; instead of specifying parameters in their airflow.cfg.&lt;/p&gt;

&lt;p&gt;We have also replaced the &lt;code&gt;executor_config&lt;/code&gt; dictionary with the &lt;code&gt;pod_override&lt;/code&gt; parameter, which takes a Kubernetes V1Pod object for a1:1 setting override. These changes have removed over three thousand lines of code from the KubernetesExecutor, which makes it run faster and creates fewer potential errors.&lt;/p&gt;

&lt;p&gt;Read more here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html?highlight=pod_override#pod-template-file&#34; target=&#34;_blank&#34;&gt;Docs on pod_template_file&lt;/a&gt; &lt;br /&gt;
&lt;a href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html?highlight=pod_override#pod-override&#34; target=&#34;_blank&#34;&gt;Docs on pod_override&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;airflow-core-and-providers-splitting-airflow-into-60-packages&#34;&gt;Airflow core and providers: Splitting Airflow into 60+ packages:&lt;/h2&gt;

&lt;p&gt;Airflow 2.0 is not a monolithic &amp;ldquo;one to rule them all&amp;rdquo; package. We’ve split Airflow into core and 61 (for now) provider packages. Each provider package is for either a particular external service (Google, Amazon, Microsoft, Snowflake), a database (Postgres, MySQL), or a protocol (HTTP/FTP). Now you can create a custom Airflow installation from &amp;ldquo;building&amp;rdquo; blocks and choose only what you need, plus add whatever other requirements you might have. Some of the common providers are installed automatically (ftp, http, imap, sqlite) as they are commonly used. Other providers are automatically installed when you choose appropriate extras when installing Airflow.&lt;/p&gt;

&lt;p&gt;The provider architecture should make it much easier to get a fully customized, yet consistent runtime with the right set of Python dependencies.&lt;/p&gt;

&lt;p&gt;But that’s not all: you can write your own custom providers and add things like custom connection types, customizations of the Connection Forms, and extra links to your operators in a manageable way. You can build your own provider and install it as a Python package and have your customizations visible right in the Airflow UI.&lt;/p&gt;

&lt;p&gt;Our very own Jarek Potiuk has written about &lt;a href=&#34;https://higrys.medium.com/airflow-2-0-providers-1bd21ba3bd93&#34; target=&#34;_blank&#34;&gt;providers in much more detail&lt;/a&gt; on Jarek&amp;rsquo;s blog.&lt;/p&gt;

&lt;p&gt;Docs on the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow-providers/&#34; target=&#34;_blank&#34;&gt;providers concept and writing custom providers&lt;/a&gt; &lt;br /&gt;
Docs on the &lt;a href=&#34;http://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html&#34; target=&#34;_blank&#34;&gt;all providers packages available&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;security&#34;&gt;Security&lt;/h2&gt;

&lt;p&gt;As part of Airflow 2.0 effort, there has been a conscious focus on Security and reducing areas of exposure. This is represented across different functional areas in different forms. For example, in the new REST API, all operations now require authorization. Similarly, in the configuration settings, the Fernet key is now required to be specified.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Configuration in the form of the airflow.cfg file has been rationalized further in distinct sections, specifically around &amp;ldquo;core&amp;rdquo;. Additionally, a significant amount of configuration options have been deprecated or moved to individual component-specific configuration files, such as the pod-template-file for Kubernetes execution-related configuration.&lt;/p&gt;

&lt;h2 id=&#34;thanks-to-all-of-you&#34;&gt;Thanks to all of you&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve tried to make as few breaking changes as possible and to provide deprecation path in the code, especially in the case of anything called in the DAG. That said, please read through UPDATING.md to check what might affect you. For example: We have re-organized the layout of operators (they now all live under airflow.providers.*) but the old names should continue to work - you&amp;rsquo;ll just notice a lot of DeprecationWarnings that need to be fixed up.&lt;/p&gt;

&lt;p&gt;Thank you so much to all the contributors who got us to this point, in no particular order: Kaxil Naik, Daniel Imberman, Jarek Potiuk, Tomek Urbaszek, Kamil Breguła, Gerard Casas Saez, Xiaodong DENG, Kevin Yang, James Timmins, Yingbo Wang, Qian Yu, Ryan Hamilton and the 100s of others who keep making Airflow better for everyone.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Journey with Airflow as an Outreachy Intern</title>
      <link>/blog/experience-with-airflow-as-an-outreachy-intern/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/experience-with-airflow-as-an-outreachy-intern/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;a href=&#34;https://www.outreachy.org/&#34; target=&#34;_blank&#34;&gt;Outreachy&lt;/a&gt; is a program which organises three months paid internships with FOSS
projects for people who are typically underrepresented in those projects.&lt;/p&gt;

&lt;h3 id=&#34;contribution-period&#34;&gt;Contribution Period&lt;/h3&gt;

&lt;p&gt;The first thing I had to do was choose a project under an organisation. After going through all the projects
I chose “Extending the REST API of Apache Airflow”, because I had a good idea of what  REST API(s) are, so I
thought it would be easier to get started with the contributions. The next step was to set up Airflow’s dev
environment which thanks to &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze&lt;/a&gt;, was a breeze.
Since I had never contributed to FOSS before so this part was overwhelming but there were plenty of issues
labelled “good first issues” with detailed descriptions and some even had code snippets so luckily that nudged
me in the right direction. These things about Airflow and the positive vibes from the community were the reasons
why I chose to stick with Airflow as my Outreachy project.&lt;/p&gt;

&lt;h3 id=&#34;internship-period&#34;&gt;Internship Period&lt;/h3&gt;

&lt;p&gt;My first PR was followed by many new experiences one of them being that I introduced a
&lt;a href=&#34;https://github.com/apache/airflow/pull/7680#issuecomment-619763051&#34; target=&#34;_blank&#34;&gt;bug&lt;/a&gt; in it;).
But nonetheless it made me familiar with the feedback loop and the feedback on my subsequent
&lt;a href=&#34;https://github.com/apache/airflow/pulls?q=is%3Apr+author%3AOmairK+&#34; target=&#34;_blank&#34;&gt;PRs&lt;/a&gt; was the focal point of the overall
learning experience I went through, which boosted my confidence to contribute more and move out of my comfort zone.
I wanted to learn more about the things that happen under the Airflow’s hood so I started filtering out recent PRs
dealing with different components and I would go through the code changes along with discussion that would help me
get a better understanding of the whole workflow. &lt;a href=&#34;https://lists.apache.org/list.html?dev@airflow.apache.org&#34; target=&#34;_blank&#34;&gt;Airflow’s mailing list&lt;/a&gt;
was also a great source of knowledge.&lt;/p&gt;

&lt;p&gt;The API related PRs that I worked on helped me with some of the important concepts like:&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&#34;https://github.com/apache/airflow/pull/9329&#34; target=&#34;_blank&#34;&gt;Pool CRUD endpoints&lt;/a&gt; where pools limit the execution parallelism.&lt;/p&gt;

&lt;p&gt;2) &lt;a href=&#34;https://github.com/apache/airflow/pull/9597&#34; target=&#34;_blank&#34;&gt;Tasks&lt;/a&gt; determine the actual work that has to be carried out.&lt;/p&gt;

&lt;p&gt;3) &lt;a href=&#34;https://github.com/apache/airflow/pull/9473&#34; target=&#34;_blank&#34;&gt;DAG&lt;/a&gt; which represents the structure for a collection
  of tasks. It keeps track of tasks, their dependencies and the sequence in which they have to run.&lt;/p&gt;

&lt;p&gt;4) &lt;a href=&#34;https://github.com/apache/airflow/pull/9473&#34; target=&#34;_blank&#34;&gt;Dag Runs&lt;/a&gt; that are the instantiation of DAG(s) in time.&lt;/p&gt;

&lt;p&gt;Through actively and passively participating in discussions I learnt that even if there is a difference of opinion
one could always learn from the different approaches, and &lt;a href=&#34;https://github.com/apache/airflow/pull/8721&#34; target=&#34;_blank&#34;&gt;this PR&lt;/a&gt; with
more than 300+ comments is the proof of it. I also started reviewing small PRs which gave me the amazing opportunity
to interact with new people. Throughout my internship I learnt a lot about different frameworks and technologies
but the biggest takeaway for me was that a code is read more often than it&amp;rsquo;s written, and I started writing code with
that in mind.&lt;/p&gt;

&lt;h3 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h3&gt;

&lt;p&gt;So with my project of extending Airflow’s REST API as well as the Outreachy internship coming to an end I would like
to thank my mentors &lt;a href=&#34;https://github.com/potiuk&#34; target=&#34;_blank&#34;&gt;Jarek Potiuk&lt;/a&gt;, &lt;a href=&#34;https://github.com/kaxil&#34; target=&#34;_blank&#34;&gt;Kaxil Naik&lt;/a&gt; and
&lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil Breguła&lt;/a&gt; for the patience and the time they invested in mentoring me and
the Airflow community for making me feel so welcomed. I plan to stick around and contribute to give back to the
community that has been made my summer, one to remember.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.12</title>
      <link>/blog/airflow-1.10.12/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.12/</guid>
      <description>
        
        
        

&lt;p&gt;Airflow 1.10.12 contains 113 commits since 1.10.11 and includes 5 new features, 23 improvements, 23 bug fixes,
and several doc changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.12/&#34; target=&#34;_blank&#34;&gt;https://pypi.org/project/apache-airflow/1.10.12/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.12/&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.12/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/changelog.html&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.12/changelog.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Airflow 1.10.11 has breaking changes with respect to
KubernetesExecutor &amp;amp; KubernetesPodOperator so I recommend users to directly upgrade to Airflow 1.10.12 instead&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/8560&#34; target=&#34;_blank&#34;&gt;Allow defining custom XCom class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9645&#34; target=&#34;_blank&#34;&gt;Get Airflow configs with sensitive data from Secret Backends&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/10282&#34; target=&#34;_blank&#34;&gt;Add AirflowClusterPolicyViolation support to Airflow local settings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;allow-defining-custom-xcom-class&#34;&gt;Allow defining Custom XCom class&lt;/h3&gt;

&lt;p&gt;Until Airflow 1.10.11, the XCom data was only stored in Airflow Metadatabase. From Airflow 1.10.12, users
would be able to define custom XCom classes. This will allow users to transfer larger data between tasks.
An example here would be to store XCom in S3 or GCS Bucket if the size of data that needs to be stored is larger
than &lt;code&gt;XCom.MAX_XCOM_SIZE&lt;/code&gt; (48 KB).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PR&lt;/strong&gt;: &lt;a href=&#34;https://github.com/apache/airflow/pull/8560&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/airflow/pull/8560&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;get-airflow-configs-with-sensitive-data-from-secret-backends&#34;&gt;Get Airflow configs with sensitive data from Secret Backends&lt;/h3&gt;

&lt;p&gt;Users would be able to get the following Airflow configs from Secrets Backend like Hashicorp Vault:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sql_alchemy_conn&lt;/code&gt; in [core] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fernet_key&lt;/code&gt; in [core] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;broker_url&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flower_basic_auth&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;result_backend&lt;/code&gt; in [celery] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;password&lt;/code&gt; in [atlas] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;smtp_password&lt;/code&gt; in [smtp] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bind_password&lt;/code&gt; in [ldap] section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git_password&lt;/code&gt; in [kubernetes] section&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further improving Airflow&amp;rsquo;s Secret Management story, from Airflow 1.10.12, users don&amp;rsquo;t need to hardcode
the &lt;strong&gt;sensitive&lt;/strong&gt; config value in airflow.cfg nor then need to use an Environment variable to set this config.&lt;/p&gt;

&lt;p&gt;For example, the metadata database connection string can either be set in airflow.cfg like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[core]
sql_alchemy_conn_secret = sql_alchemy_conn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will retrieve config option from the set Secret Backends.&lt;/p&gt;

&lt;p&gt;As you can see you just need to add a &lt;code&gt;_secret&lt;/code&gt; suffix at the end of the actual config option
and the value needs to be the &lt;strong&gt;key&lt;/strong&gt; which the Secrets backend will look for.&lt;/p&gt;

&lt;p&gt;Similarly, &lt;code&gt;_secret&lt;/code&gt; config options can also be set using a corresponding environment variable. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export AIRFLOW__CORE__SQL_ALCHEMY_CONN_SECRET=sql_alchemy_conn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/howto/set-config.html&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.12/howto/set-config.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;add-airflowclusterpolicyviolation-support-to-airflow-local-settings-py&#34;&gt;Add AirflowClusterPolicyViolation support to airflow_local_settings.py&lt;/h3&gt;

&lt;p&gt;Users can use Cluster Policies to apply cluster-wide checks on Airflow
tasks. You can raise &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/_api/airflow/exceptions/index.html#airflow.exceptions.AirflowClusterPolicyViolation&#34; target=&#34;_blank&#34;&gt;AirflowClusterPolicyViolation&lt;/a&gt;
in a policy or task mutation hook to prevent a DAG from being
imported or prevent a task from being executed if the task is not compliant with
your check.&lt;/p&gt;

&lt;p&gt;These checks are intended to help teams using Airflow to protect against common
beginner errors that may get past a code reviewer, rather than as technical
security controls.&lt;/p&gt;

&lt;p&gt;For example, don&amp;rsquo;t run tasks without &lt;code&gt;airflow&lt;/code&gt; owners:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def task_must_have_owners(task):
    if not task.owner or task.owner.lower() == conf.get(&#39;operators&#39;, &#39;default_owner&#39;):
        raise AirflowClusterPolicyViolation(
            &#39;Task must have non-None non-default owner. Current value: {}&#39;.format(task.owner))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.12/concepts.html#cluster-policies-for-custom-task-checks&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.12/concepts.html#cluster-policies-for-custom-task-checks&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;launch-pods-via-yaml-files-when-using-kubernetesexecutor-and-kubernetespodoperator&#34;&gt;Launch Pods via YAML files when using KubernetesExecutor and KubernetesPodOperator&lt;/h3&gt;

&lt;p&gt;As of 1.10.12, users can launch pods via YAML files instead of passing various configurations.&lt;/p&gt;

&lt;p&gt;To allow greater flexibility we have deprecated Airflow&amp;rsquo;s Pod class and instead now use classes and
objects from the official Kubernetes API. The POD class will still work but raise a deprecation
warning. This feature involved a pretty extensive rewrite of all of our pod creation code.&lt;/p&gt;

&lt;p&gt;Initially, we were going to hold off on these features until Airflow 2.0. However, we soon
realized that exposing these features in 1.10.x is crucial in preparing users for the 2.0 release to come.&lt;/p&gt;

&lt;p&gt;Details: &lt;a href=&#34;https://github.com/apache/airflow/pull/6230&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/airflow/pull/6230&lt;/a&gt; (&lt;a href=&#34;https://github.com/apache/airflow/commit/7aa0f472b57985a952a3e3d0a38f1b2535d93413&#34; target=&#34;_blank&#34;&gt;Backport commit&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&#34;updating-guide&#34;&gt;Updating Guide&lt;/h2&gt;

&lt;p&gt;If you are updating Apache Airflow from a previous version to &lt;code&gt;1.10.12&lt;/code&gt;, please take a note of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;airflow upgradedb&lt;/code&gt; after &lt;code&gt;pip install -U apache-airflow==1.10.12&lt;/code&gt; as &lt;code&gt;1.10.12&lt;/code&gt; contains 1 database migration.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As of airflow 1.10.12, using the &lt;code&gt;airflow.contrib.kubernetes.Pod&lt;/code&gt; class in the &lt;code&gt;pod_mutation_hook&lt;/code&gt; is now
deprecated. Instead we recommend that users treat the pod parameter as a &lt;code&gt;kubernetes.client.models.V1Pod&lt;/code&gt; object.
This means that users now have access to the full Kubernetes API when modifying airflow pods for mutating POD.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Previously, when tasks skipped by SkipMixin (such as &lt;code&gt;BranchPythonOperator&lt;/code&gt;, &lt;code&gt;BaseBranchOperator&lt;/code&gt; and
&lt;code&gt;ShortCircuitOperator&lt;/code&gt;) are cleared, they execute. Since 1.10.12, when such skipped tasks are cleared,
they will be skipped again by the newly introduced &lt;code&gt;NotPreviouslySkippedDep&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;special-note&#34;&gt;Special Note&lt;/h2&gt;

&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;

&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;

&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;

&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;

&lt;p&gt;Airflow 1.10.12 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;

&lt;p&gt;The Flask-AppBuilder (FAB) based UI allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in
your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;

&lt;h3 id=&#34;we-have-moved-to-github-issues&#34;&gt;We have moved to GitHub Issues&lt;/h3&gt;

&lt;p&gt;The Airflow Project has moved from &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues&#34; target=&#34;_blank&#34;&gt;JIRA&lt;/a&gt; to
&lt;a href=&#34;https://github.com/apache/airflow/issues&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; for tracking issues.&lt;/p&gt;

&lt;p&gt;So if you find any bugs in Airflow 1.10.12 please create a GitHub Issue for it.&lt;/p&gt;

&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;

&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.12 release. Thank you to all contributors!&lt;/p&gt;

&lt;p&gt;Alexander Sutcliffe, Andy, Aneesh Joseph, Ash Berlin-Taylor, Aviral Agrawal, BaoshanGu, Beni Ben zikry,
Daniel Imberman, Daniel Standish, Danylo Baibak, Ephraim Anierobi, Felix Uellendall, Greg Neiheisel,
Hartorn, Jacob Ferriero, Jannik F, Jarek Potiuk, Jinhui Zhang, Kamil Breguła, Kaxil Naik, Kurganov,
Luis Magana, Max Arrich, Pete DeJoy, Sumit Maheshwari, Tomek Urbaszek, Vicken Simonian, Vinnie Guimaraes,
William Tran, Xiaodong Deng, YI FU, Zikun Zhu, dewaldabrie, pulsar314, retornam, yuqian90&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow For Newcomers</title>
      <link>/blog/apache-airflow-for-newcomers/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/apache-airflow-for-newcomers/</guid>
      <description>
        
        
        

&lt;p&gt;Apache Airflow is a platform to programmatically author, schedule, and monitor workflows.
A workflow is a sequence of tasks that processes a set of data. You can think of workflow as the
path that describes how tasks go from being undone to done. Scheduling, on the other hand, is the
process of planning, controlling, and optimizing when a particular task should be done.&lt;/p&gt;

&lt;h3 id=&#34;authoring-workflow-in-apache-airflow&#34;&gt;Authoring Workflow in Apache Airflow.&lt;/h3&gt;

&lt;p&gt;Airflow makes it easy to author workflows using python scripts. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34;&gt;Directed Acyclic Graph&lt;/a&gt;
(DAG) represents a workflow in Airflow. It is a collection of tasks in a way that shows each task&amp;rsquo;s
relationships and dependencies. You can have as many DAGs as you want, and Airflow will execute
them according to the task&amp;rsquo;s relationships and dependencies. If task B depends on the successful
execution of another task A, it means Airflow will run task A and only run task B after task A.
This dependency is very easy to express in Airflow. For example, the above scenario is expressed as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;task_A &amp;gt;&amp;gt; task_B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also equivalent to&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;task_A.set_downstream(task_B)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Simple_dag.png&#34; alt=&#34;Simple Dag&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That helps Airflow to know that it needs to execute task A before task B. Tasks can have far more complex
relationships to each other than expressed above and Airflow figures out how and when to execute the tasks following
their relationships and dependencies.
&lt;img src=&#34;semicomplex.png&#34; alt=&#34;Complex Dag&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before we discuss the architecture of Airflow that makes scheduling, executing, and monitoring of
workflow an easy thing, let us discuss the &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze environment&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;breeze-environment&#34;&gt;Breeze Environment&lt;/h3&gt;

&lt;p&gt;The breeze environment is the development environment for Airflow where you can run tests, build images,
build documentations and so many other things. There are excellent
&lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;documentation and video&lt;/a&gt; on Breeze environment.
Please check them out. You enter the Breeze environment by running the &lt;code&gt;./breeze&lt;/code&gt; script. You can run all
the commands mentioned here in the Breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;scheduler&#34;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;The scheduler is the component that monitors DAGs and triggers those tasks whose dependencies have
been met. It watches over the DAG folder, checking the tasks in each DAG and triggers them once they
are ready. It accomplishes this by spawning a process that runs periodically(every minute or so)
reading the metadata database to check the status of each task and decides what needs to be done.
The metadata database is where the status of all tasks are recorded. The status can be one of running,
 success, failed, etc.&lt;/p&gt;

&lt;p&gt;A task is said to be ready when its dependencies have been met. The dependencies include all the data
necessary for the task to be executed. It should be noted that the scheduler won&amp;rsquo;t trigger your tasks until
the period it covers has ended. If a task&amp;rsquo;s &lt;code&gt;schedule_interval&lt;/code&gt; is &lt;code&gt;@daily&lt;/code&gt;, the scheduler triggers the task
at the end of the day and not at the beginning. This is to ensure that the necessary data needed for the tasks
are ready. It is also possible to trigger tasks manually on the UI.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze environment&lt;/a&gt;, the scheduler is started by running the command &lt;code&gt;airflow scheduler&lt;/code&gt;. It uses
the configured production environment. The configuration can be specified in &lt;code&gt;airflow.cfg&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;executor&#34;&gt;Executor&lt;/h3&gt;

&lt;p&gt;Executors are responsible for running tasks. They work with the scheduler to get information about
what resources are needed to run a task as the task is queued.&lt;/p&gt;

&lt;p&gt;By default, Airflow uses the &lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/sequential.html#sequential-executor&#34; target=&#34;_blank&#34;&gt;SequentialExecutor&lt;/a&gt;.
 However, this executor is limited and it is the only executor that can be used with SQLite.&lt;/p&gt;

&lt;p&gt;There are many other &lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/index.html&#34; target=&#34;_blank&#34;&gt;executors&lt;/a&gt;,
 the difference is on the resources they have and how they choose to use the resources. The available executors
 are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sequential Executor&lt;/li&gt;
&lt;li&gt;Debug Executor&lt;/li&gt;
&lt;li&gt;Local Executor&lt;/li&gt;
&lt;li&gt;Dask Executor&lt;/li&gt;
&lt;li&gt;Celery Executor&lt;/li&gt;
&lt;li&gt;Kubernetes Executor&lt;/li&gt;
&lt;li&gt;Scaling Out with Mesos (community contributed)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CeleryExecutor is a better executor compared to the SequentialExecutor. The CeleryExecutor uses several
workers to execute a job in a distributed way. If a worker node is ever down, the CeleryExecutor assign its
task to another worker node. This ensures high availability.&lt;/p&gt;

&lt;p&gt;The CeleryExecutor works closely with the scheduler which adds a message to the queue and the Celery broker
which delivers the message to a Celery worker to execute.
You can find more information about the CeleryExecutor and how to configure it at the
&lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/celery.html#celery-executor&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;webserver&#34;&gt;Webserver&lt;/h3&gt;

&lt;p&gt;The webserver is the web interface (UI) for Airflow. The UI is feature-rich. It makes it easy to
monitor and troubleshoot DAGs and Tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;airflow-ui.png&#34; alt=&#34;airflow UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are many actions you can perform on the UI. You can trigger a task, monitor the execution
including the duration of the task. The UI makes it possible to view the task&amp;rsquo;s dependencies in a
tree view and graph view. You can view task logs in the UI.&lt;/p&gt;

&lt;p&gt;The web UI is started with the command &lt;code&gt;airflow webserver&lt;/code&gt; in the breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;backend&#34;&gt;Backend&lt;/h3&gt;

&lt;p&gt;By default, Airflow uses the SQLite backend for storing the configuration information, DAG states,
and much other useful information. This should not be used in production as SQLite can cause a data
loss.&lt;/p&gt;

&lt;p&gt;You can use PostgreSQL or MySQL as a backend for airflow. It is easy to change to PostgreSQL or MySQL.&lt;/p&gt;

&lt;p&gt;The command &lt;code&gt;./breeze --backend mysql&lt;/code&gt; selects MySQL as the backend when starting the breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;operators&#34;&gt;Operators&lt;/h3&gt;

&lt;p&gt;Operators determine what gets done by a task. Airflow has a lot of builtin Operators. Each operator
does a specific task. There&amp;rsquo;s a BashOperator that executes a bash command, the PythonOperator which
calls a python function, AwsBatchOperator which executes a job on AWS Batch and &lt;a href=&#34;https://airflow.apache.org/docs/stable/concepts.html#operators&#34; target=&#34;_blank&#34;&gt;many more&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;sensors&#34;&gt;Sensors&lt;/h4&gt;

&lt;p&gt;Sensors can be described as special operators that are used to monitor a long-running task.
Just like Operators, there are many predefined sensors in Airflow. These includes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AthenaSensor: Asks for the state of the Query until it reaches a failure state or success state.&lt;/li&gt;
&lt;li&gt;AzureCosmosDocumentSensor: Checks for the existence of a document which matches the given query in CosmosDB&lt;/li&gt;
&lt;li&gt;GoogleCloudStorageObjectSensor:  Checks for the existence of a file in Google Cloud Storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A list of most of the available sensors can be found in this &lt;a href=&#34;https://airflow.apache.org/docs/stable/_api/airflow/contrib/sensors/index.html?highlight=sensors#module-airflow.contrib.sensors&#34; target=&#34;_blank&#34;&gt;module&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contributing-to-airflow&#34;&gt;Contributing to Airflow&lt;/h3&gt;

&lt;p&gt;Airflow is an open source project, everyone is welcome to contribute. It is easy to get started thanks
to the excellent &lt;a href=&#34;https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst&#34; target=&#34;_blank&#34;&gt;documentation on how to get started&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I joined the community about 12 weeks ago through the &lt;a href=&#34;https://www.outreachy.org/&#34; target=&#34;_blank&#34;&gt;Outreachy Program&lt;/a&gt; and have
completed about &lt;a href=&#34;https://github.com/apache/airflow/pulls/ephraimbuddy&#34; target=&#34;_blank&#34;&gt;40 PRs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It has been an amazing experience! Thanks to my mentors &lt;a href=&#34;https://github.com/potiuk&#34; target=&#34;_blank&#34;&gt;Jarek&lt;/a&gt; and
&lt;a href=&#34;https://github.com/kaxil&#34; target=&#34;_blank&#34;&gt;Kaxil&lt;/a&gt;, and the community members especially &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt;
and &lt;a href=&#34;https://github.com/turbaszek&#34; target=&#34;_blank&#34;&gt;Tomek&lt;/a&gt; for all their support. I&amp;rsquo;m grateful!&lt;/p&gt;

&lt;p&gt;Thank you so much, &lt;a href=&#34;https://github.com/leahecole&#34; target=&#34;_blank&#34;&gt;Leah E. Cole&lt;/a&gt;, for your wonderful reviews.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Implementing Stable API for Apache Airflow</title>
      <link>/blog/implementing-stable-api-for-apache-airflow/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/implementing-stable-api-for-apache-airflow/</guid>
      <description>
        
        
        

&lt;p&gt;My &lt;a href=&#34;https://outreachy.org&#34; target=&#34;_blank&#34;&gt;Outreachy internship&lt;/a&gt; is coming to its ends which is also the best time to look back and
reflect on the progress so far.&lt;/p&gt;

&lt;p&gt;The goal of my project is to Extend and Improve the Apache Airflow REST API. In this post,
I will be sharing my progress so far.&lt;/p&gt;

&lt;p&gt;We started a bit late implementing the REST API because it took time for the OpenAPI 3.0
specification we were to use for the project to be merged. Thanks to &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt;,
who paved the way for us to start implementing the REST API endpoints. Below are the endpoints I
implemented and the challenges I encountered, including how I overcame them.&lt;/p&gt;

&lt;h3 id=&#34;implementing-the-read-only-connection-endpoints&#34;&gt;Implementing The Read-Only Connection Endpoints&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/apache/airflow/pull/9095&#34; target=&#34;_blank&#34;&gt;read-only connection endpoints&lt;/a&gt; were the first endpoint I implemented. Looking back,
I can see how much I have improved.&lt;/p&gt;

&lt;p&gt;I started by implementing the database schema for the Connection table using &lt;a href=&#34;https://marshmallow.readthedocs.io/en/2.x-line/&#34; target=&#34;_blank&#34;&gt;Marshmallow 2&lt;/a&gt;.
We had to use Marshmallow 2 because Flask-AppBuilder was still using it and Flask-AppBuilder
is deeply integrated to Apache Airflow. This meant I had to unlearn Marshmallow 3 that I had
 been studying before this realization, but thankfully, &lt;a href=&#34;https://marshmallow.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34;&gt;Marshmallow 3&lt;/a&gt; isn&amp;rsquo;t too
 different, so I was able to start using Marshmallow 2 in no time.&lt;/p&gt;

&lt;p&gt;This first PR would have been more difficult than it was unless there had been any reference
endpoint to look at. &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt; implemented a &lt;a href=&#34;https://github.com/apache/airflow/pull/9045&#34; target=&#34;_blank&#34;&gt;draft PR&lt;/a&gt; in which I took inspiration from.
Thanks to this, It was easy for me to write the unit tests. It was also in this endpoint that
 I learned using &lt;a href=&#34;https://github.com/wolever/parameterized&#34; target=&#34;_blank&#34;&gt;parameterized&lt;/a&gt; in unit tests :D.&lt;/p&gt;

&lt;h3 id=&#34;implementing-the-read-only-dagruns-endpoints&#34;&gt;Implementing The Read-Only DagRuns Endpoints&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&#34;https://github.com/apache/airflow/pull/9153&#34; target=&#34;_blank&#34;&gt;endpoint&lt;/a&gt; came with its many challenges, especially on filtering with &lt;code&gt;datetimes&lt;/code&gt;.
This was because the &lt;code&gt;connexion&lt;/code&gt; library we were using to build the REST API was not validating
date-time format in OpenAPI 3.0 specification, what I eventually found out, was intentional.
Connexion dropped &lt;code&gt;strict-rfc3339&lt;/code&gt; because of the later license which is not compatible with
Apache 2.0 license.&lt;/p&gt;

&lt;p&gt;I implemented a workaround on this, by defining a function called &lt;code&gt;conn_parse_datetime&lt;/code&gt; in the
API utils module. This was later refactored and thankfully, &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt;
 implemented a decorator that allowed us to have cleaner code on the views while using this function.&lt;/p&gt;

&lt;p&gt;Then we tried using &lt;code&gt;rfc3339-validator&lt;/code&gt; whose license is compatible with Apache 2.0 licence but
 later discarded this because with our custom date parser we were able to use duration and
 not just date times.&lt;/p&gt;

&lt;h3 id=&#34;other-endpoints&#34;&gt;Other Endpoints&lt;/h3&gt;

&lt;p&gt;I implemented some different other endpoints. One peculiar issue I faced was because of Marshmallow 2
not giving error when extra fields are in the request body. I implemented a &lt;code&gt;validate_unknown&lt;/code&gt;
method on the schema to handle this. Thankfully, Flask-AppBuilder updated to using Marshmallow 3,
we quickly updated Flask-AppBuilder in Apache Airflow and started using Marshmallow 3 too.&lt;/p&gt;

&lt;p&gt;Here are some PRs I contributed that are related to the REST API:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9227&#34; target=&#34;_blank&#34;&gt;Add event log endpoints&lt;/a&gt;
The event log would help users get information on operations performed at the UI&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9266&#34; target=&#34;_blank&#34;&gt;Add CRUD endpoints for connection&lt;/a&gt;
This PR performs DELETE, PATCH and POST operations on &lt;code&gt;Connection&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9331&#34; target=&#34;_blank&#34;&gt;Add log endpoint&lt;/a&gt;
This PR enables users to get Task Instances log entries&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9431&#34; target=&#34;_blank&#34;&gt;Move limit &amp;amp; offset to kwargs in views plus work on a configurable maximum limit&lt;/a&gt;
This helped us in having a neat code on the views and added configurable maximum limit on query results.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9648&#34; target=&#34;_blank&#34;&gt;Update FlaskAppBuilder to v3&lt;/a&gt;
This enabled Airflow to start using v3 of Flask App Builder and also made it possible for the API to use
 a modern database serializer/deserializer&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/9771&#34; target=&#34;_blank&#34;&gt;Add migration guide from the experimental REST API to the stable REST API&lt;/a&gt;
This would enable users to start using the stable REST API in less time.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;follow-ups&#34;&gt;Follow-Ups&lt;/h3&gt;

&lt;p&gt;There is still lots of works to be done on the REST API including writing helpful documentation.
I still follow up on these and hopefully, we will complete the REST API before the internship ends.&lt;/p&gt;

&lt;p&gt;I am very grateful to my mentors, &lt;a href=&#34;https://github.com/potiuk&#34; target=&#34;_blank&#34;&gt;Jarek&lt;/a&gt; and &lt;a href=&#34;https://github.com/kaxil&#34; target=&#34;_blank&#34;&gt;Kaxil&lt;/a&gt; for their
patience with me and for surviving my never-ending questions. &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt; and &lt;a href=&#34;https://github.com/turbaszek&#34; target=&#34;_blank&#34;&gt;Tomek&lt;/a&gt;
have been very supportive and I appreciate them for their support and amazing code reviews.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/leahecole&#34; target=&#34;_blank&#34;&gt;Leah E. Cole&lt;/a&gt; and &lt;a href=&#34;https://github.com/mschickensoup&#34; target=&#34;_blank&#34;&gt;Karolina Rosół&lt;/a&gt;, for their
wonderful reviews. I&amp;rsquo;m grateful.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.10</title>
      <link>/blog/airflow-1.10.10/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.10/</guid>
      <description>
        
        
        

&lt;p&gt;Airflow 1.10.10 contains 199 commits since 1.10.9 and includes 11 new features, 43 improvements, 44 bug fixes, and several doc changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.10/&#34; target=&#34;_blank&#34;&gt;https://pypi.org/project/apache-airflow/1.10.10/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.10/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/changelog.html&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.10/changelog.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/8046&#34; target=&#34;_blank&#34;&gt;Allow user to chose timezone to use in the RBAC UI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7832&#34; target=&#34;_blank&#34;&gt;Add Production Docker image support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&#34; target=&#34;_blank&#34;&gt;Allow Retrieving Airflow Connections &amp;amp; Variables from various Secrets backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&#34; target=&#34;_blank&#34;&gt;Stateless Webserver using DAG Serialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7880&#34; target=&#34;_blank&#34;&gt;Tasks with Dummy Operators are no longer sent to executor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7312&#34; target=&#34;_blank&#34;&gt;Allow passing DagRun conf when triggering dags via UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;allow-user-to-chose-timezone-to-use-in-the-rbac-ui&#34;&gt;Allow user to chose timezone to use in the RBAC UI&lt;/h3&gt;

&lt;p&gt;By default the Web UI will show times in UTC. It is possible to change the timezone shown by using the menu in the top
 right (click on the clock to activate it):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;rbac-ui-timezone.gif&#34; alt=&#34;Allow user to chose timezone to use in the RBAC UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This feature is only available for the RBAC UI (enabled using &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&#34;add-production-docker-image-support&#34;&gt;Add Production Docker image support&lt;/h3&gt;

&lt;p&gt;There are brand new production images (alpha quality) available for Airflow 1.10.10. You can pull them from the
&lt;a href=&#34;https://hub.docker.com/r/apache/airflow&#34; target=&#34;_blank&#34;&gt;Apache Airflow Dockerhub&lt;/a&gt; repository and start using it.&lt;/p&gt;

&lt;p&gt;More information about using production images can be found in &lt;a href=&#34;https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images&lt;/a&gt;. Soon it will be updated with
information how to use images using official helm chart.&lt;/p&gt;

&lt;p&gt;To pull the images you can run one of the following commands:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python2.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10-python3.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker pull apache/airflow:1.10.10&lt;/code&gt; (uses Python 3.6)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;allow-retrieving-airflow-connections-variables-from-various-secrets-backend&#34;&gt;Allow Retrieving Airflow Connections &amp;amp; Variables from various Secrets backend&lt;/h3&gt;

&lt;p&gt;From Airflow 1.10.10, users would be able to get Airflow Variables from Environment Variables.&lt;/p&gt;

&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A new concept of Secrets Backend has been introduced to retrieve Airflow Connections and Variables.&lt;/p&gt;

&lt;p&gt;From Airflow 1.10.10, users can retrieve Connections &amp;amp; Variables using the same syntax (no DAG code change is required),
from a secret backend defined in &lt;code&gt;airflow.cfg&lt;/code&gt;. If no backend is defined, Airflow falls-back to Environment Variables
and then Metadata DB.&lt;/p&gt;

&lt;p&gt;Check &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html#configuration&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html#configuration&lt;/a&gt; for details on how-to
configure Secrets backend.&lt;/p&gt;

&lt;p&gt;As of 1.10.10, Airflow supports the following Secret Backends:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hashicorp Vault&lt;/li&gt;
&lt;li&gt;GCP Secrets Manager&lt;/li&gt;
&lt;li&gt;AWS Parameters Store&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Example configuration to use Hashicorp Vault as the backend:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[secrets]
backend = airflow.contrib.secrets.hashicorp_vault.VaultBackend
backend_kwargs = {&amp;quot;url&amp;quot;: &amp;quot;http://127.0.0.1:8200&amp;quot;, &amp;quot;connections_path&amp;quot;: &amp;quot;connections&amp;quot;, &amp;quot;variables_path&amp;quot;: &amp;quot;variables&amp;quot;, &amp;quot;mount_point&amp;quot;: &amp;quot;airflow&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stateless-webserver-using-dag-serialization&#34;&gt;Stateless Webserver using DAG Serialization&lt;/h3&gt;

&lt;p&gt;The Webserver can now run without access to DAG Files when DAG Serialization is turned on.
The 2 limitations we had in 1.10.7-1.10.9 (
&lt;a href=&#34;https://airflow.apache.org/docs/1.10.7/dag-serialization.html#limitations)&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.7/dag-serialization.html#limitations)&lt;/a&gt;
have been resolved.&lt;/p&gt;

&lt;p&gt;The main advantage of this would be reduction in Webserver startup time for large number of DAGs.
Without DAG Serialization all the DAGs are loaded in the DagBag during the
Webserver startup.&lt;/p&gt;

&lt;p&gt;With DAG Serialization, an empty DagBag is created and
Dags are loaded from DB only when needed (i.e. when a particular DAG is
clicked on in the home page)&lt;/p&gt;

&lt;p&gt;Details: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.10/dag-serialization.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;tasks-using-dummy-operators-are-no-longer-sent-to-executor&#34;&gt;Tasks using Dummy Operators are no longer sent to executor&lt;/h3&gt;

&lt;p&gt;The Dummy operators does not actually do any work and are mostly used for organizing/grouping tasks along
with BranchPythonOperator.&lt;/p&gt;

&lt;p&gt;Previously, when using Kubernetes Executor, the executor would spin up a whole worker pod to execute a dummy task.
With Airflow 1.10.10 tasks using Dummy Operators would be scheduled &amp;amp; evaluated by the Scheduler but not sent to the
Executor. This should significantly improve execution time and resource usage.&lt;/p&gt;

&lt;h3 id=&#34;allow-passing-dagrun-conf-when-triggering-dags-via-ui&#34;&gt;Allow passing DagRun conf when triggering dags via UI&lt;/h3&gt;

&lt;p&gt;When triggering a DAG from the CLI or the REST API, it s possible to pass configuration for the DAG run as a JSON blob.&lt;/p&gt;

&lt;p&gt;From Airflow 1.10.10, when a user clicks on Trigger Dag button, a new screen confirming the trigger request, and allowing the user to pass a JSON configuration
blob would be show.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;trigger-dag-conf.png&#34; alt=&#34;Allow passing DagRun conf when triggering dags via UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Details: &lt;a href=&#34;https://github.com/apache/airflow/pull/7312&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/airflow/pull/7312&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;updating-guide&#34;&gt;Updating Guide&lt;/h2&gt;

&lt;p&gt;If you are updating Apache Airflow from a previous version to &lt;code&gt;1.10.10&lt;/code&gt;, please take a note of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;airflow upgradedb&lt;/code&gt; after &lt;code&gt;pip install -U apache-airflow==1.10.10&lt;/code&gt; as &lt;code&gt;1.10.10&lt;/code&gt; contains 3 database migrations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you have used &lt;code&gt;none_failed&lt;/code&gt; trigger rule in your DAG, change it to use the new &lt;code&gt;none_failed_or_skipped&lt;/code&gt; trigger rule.
As previously implemented, the actual behavior of &lt;code&gt;none_failed&lt;/code&gt; trigger rule would skip the current task if all parents of the task
had also skipped. This was not in-line with what was documented about that trigger rule. We have changed the implementation to match
the documentation, hence if you need the old behavior use &lt;code&gt;none_failed_or_skipped&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;More details in &lt;a href=&#34;https://github.com/apache/airflow/pull/7464&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/airflow/pull/7464&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Setting empty string to a Airflow Variable will now return an empty string, it previously returned &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; Variable.set(&#39;test_key&#39;, &#39;&#39;)
&amp;gt;&amp;gt; Variable.get(&#39;test_key&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code returned &lt;code&gt;None&lt;/code&gt; previously, now it will return &amp;ldquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When a task is marked as &lt;code&gt;success&lt;/code&gt; by a user in Airflow UI, function defined in &lt;code&gt;on_success_callback&lt;/code&gt; will be called.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;special-note-deprecations&#34;&gt;Special Note / Deprecations&lt;/h2&gt;

&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;

&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;

&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;

&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;

&lt;p&gt;Airflow 1.10.10 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;

&lt;p&gt;The Flask-AppBuilder (FAB) based UI allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;

&lt;h3 id=&#34;running-airflow-on-macos&#34;&gt;Running Airflow on MacOS&lt;/h3&gt;

&lt;p&gt;Run &lt;code&gt;export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES&lt;/code&gt; in your scheduler environmentIf you are running Airflow on MacOS
and get the following error in the Scheduler logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;objc[1873]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[1873]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This error occurs because of added security to restrict multiprocessing &amp;amp; multithreading in Mac OS High Sierra and above.&lt;/p&gt;

&lt;h3 id=&#34;we-have-moved-to-github-issues&#34;&gt;We have moved to GitHub Issues&lt;/h3&gt;

&lt;p&gt;The Airflow Project has moved from &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues&#34; target=&#34;_blank&#34;&gt;JIRA&lt;/a&gt; to
&lt;a href=&#34;https://github.com/apache/airflow/issues&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; for tracking issues.&lt;/p&gt;

&lt;p&gt;So if you find any bugs in Airflow 1.10.10 please create a GitHub Issue for it.&lt;/p&gt;

&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;

&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.10 release. Thank you to all contributors!&lt;/p&gt;

&lt;p&gt;ANiteckiP, Alex Guziel, Alex Lue, Anita Fronczak, Ash Berlin-Taylor, Benji Visser, Bhavika Tekwani, Brad Dettmer, Chris McLennon, Cooper Gillan, Daniel Imberman, Daniel Standish, Felix Uellendall, Jarek Potiuk, Jiajie Zhong, Jithin Sukumar, Kamil Breguła, Kaxil Naik, Kengo Seki, Kris, Kumpan Anton, Lokesh Lal, Louis Guitton, Louis Simoneau, Luyao Yang, Noël Bardelot, Omair Khan, Philipp Großelfinger, Ping Zhang, RasPavel, Ray, Robin Edwards, Ry Walker, Saurabh, Sebastian Brandt, Tomek Kzukowski, Tomek Urbaszek, Van-Duyet Le, Xiaodong Deng, Xinbin Huang, Yu Qian, Zacharya, atrbgithub, cong-zhu, retornam&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow 1.10.8 &amp; 1.10.9</title>
      <link>/blog/airflow-1.10.8-1.10.9/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-1.10.8-1.10.9/</guid>
      <description>
        
        
        

&lt;p&gt;Airflow 1.10.8 contains 160 commits since 1.10.7 and includes 4 new features, 42 improvements, 36 bug fixes, and several doc changes.&lt;/p&gt;

&lt;p&gt;We released 1.10.9 on the same day as one of the Flask dependencies (Werkzeug) released 1.0 which broke Airflow 1.10.8.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyPI&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/apache-airflow/1.10.9/&#34; target=&#34;_blank&#34;&gt;https://pypi.org/project/apache-airflow/1.10.9/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://airflow.apache.org/docs/1.10.9/&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/docs/1.10.9/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog (1.10.8)&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changelog (1.10.9)&lt;/strong&gt;: &lt;a href=&#34;http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10&#34; target=&#34;_blank&#34;&gt;http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of the noteworthy new features (user-facing) are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/6489&#34; target=&#34;_blank&#34;&gt;Add tags to DAGs and use it for filtering in the UI (RBAC only)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://airflow.apache.org/docs/1.10.9/executor/debug.html&#34; target=&#34;_blank&#34;&gt;New Executor: DebugExecutor for Local debugging from your IDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7281&#34; target=&#34;_blank&#34;&gt;Allow passing conf in &amp;ldquo;Add DAG Run&amp;rdquo; (Triggered Dags) view&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/7038&#34; target=&#34;_blank&#34;&gt;Allow dags to run for future execution dates for manually triggered DAGs (only if &lt;code&gt;schedule_interval=None&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.apache.org/docs/1.10.9/configurations-ref.html&#34; target=&#34;_blank&#34;&gt;Dedicated page in documentation for all configs in airflow.cfg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;add-tags-to-dags-and-use-it-for-filtering-in-the-ui&#34;&gt;Add tags to DAGs and use it for filtering in the UI&lt;/h3&gt;

&lt;p&gt;In order to filter DAGs (e.g by team), you can add tags in each dag. The filter is saved in a cookie and can be reset by the reset button.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;In your Dag file, pass a list of tags you want to add to DAG object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dag = DAG(
    dag_id=&#39;example_dag_tag&#39;,
    schedule_interval=&#39;0 0 * * *&#39;,
    tags=[&#39;example&#39;]
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Screenshot&lt;/strong&gt;:
&lt;img src=&#34;airflow-dag-tags.png&#34; alt=&#34;Add filter by DAG tags&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This feature is only available for the RBAC UI (enabled using &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;).&lt;/p&gt;

&lt;h2 id=&#34;special-note-deprecations&#34;&gt;Special Note / Deprecations&lt;/h2&gt;

&lt;h3 id=&#34;python-2&#34;&gt;Python 2&lt;/h3&gt;

&lt;p&gt;Python 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.
Airflow 1.10.* would be the last series to support Python 2.&lt;/p&gt;

&lt;p&gt;We strongly recommend users to use Python &amp;gt;= 3.6&lt;/p&gt;

&lt;h3 id=&#34;use-airflow-rbac-ui&#34;&gt;Use Airflow RBAC UI&lt;/h3&gt;

&lt;p&gt;Airflow 1.10.9 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.&lt;/p&gt;

&lt;p&gt;The Flask-AppBuilder (FAB) based UI is allows Role-based Access Control and has more advanced features compared to
the legacy Flask-admin based UI. This UI can be enabled by setting &lt;code&gt;rbac=True&lt;/code&gt; in &lt;code&gt;[webserver]&lt;/code&gt; section in your &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Flask-admin based UI is deprecated and new features won&amp;rsquo;t be ported to it. This UI will still be the default
for 1.10.* series but would no longer be available from Airflow 2.0&lt;/p&gt;

&lt;h2 id=&#34;list-of-contributors&#34;&gt;List of Contributors&lt;/h2&gt;

&lt;p&gt;According to git shortlog, the following people contributed to the 1.10.8 and 1.10.9 release. Thank you to all contributors!&lt;/p&gt;

&lt;p&gt;Anita Fronczak, Ash Berlin-Taylor, BasPH, Bharat Kashyap, Bharath Palaksha, Bhavika Tekwani, Bjorn Olsen, Brian Phillips, Cooper Gillan, Daniel Cohen, Daniel Imberman, Daniel Standish, Gabriel Eckers, Hossein Torabi, Igor Khrol, Jacob, Jarek Potiuk, Jay, Jiajie Zhong, Jithin Sukumar, Kamil Breguła, Kaxil Naik, Kousuke Saruta, Mustafa Gök, Noël Bardelot, Oluwafemi Sule, Pete DeJoy, QP Hou, Qian Yu, Robin Edwards, Ry Walker, Steven van Rossum, Tomek Urbaszek, Xinbin Huang, Yuen-Kuei Hsueh, Yu Qian, Zacharya, ZxMYS, rconroy293, tooptoop4&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Experience in Google Season of Docs 2019 with Apache Airflow</title>
      <link>/blog/experience-in-google-season-of-docs-2019-with-apache-airflow/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/experience-in-google-season-of-docs-2019-with-apache-airflow/</guid>
      <description>
        
        
        

&lt;p&gt;I came across &lt;a href=&#34;https://developers.google.com/season-of-docs&#34; target=&#34;_blank&#34;&gt;Google Season of Docs&lt;/a&gt; (GSoD) almost by accident, thanks to my extensive HackerNews and Twitter addiction.  I was familiar with the Google Summer of Code but not with this program.
It turns out it was the inaugural phase. I read the details, and the process felt a lot like GSoC except that this was about documentation.&lt;/p&gt;

&lt;h2 id=&#34;about-me&#34;&gt;About Me&lt;/h2&gt;

&lt;p&gt;I have been writing tech articles on medium as well as my blog for the past 1.5 years.  Blogging helps me test my understanding of the concepts as untangling the toughest of ideas in simple sentences requires a considerable time investment.&lt;/p&gt;

&lt;p&gt;Also, I have been working as a Software Developer for the past three years, which involves writing documentation for my projects as well. I completed my B.Tech from  IIT Roorkee. During my stay in college, I applied for GSoC once but didn’t make it through in the final list of selected candidates.&lt;/p&gt;

&lt;p&gt;I saw GSoD as an excellent opportunity to improve my technical writing skills using feedback from the open-source community. I contributed some bug fixes and features to Apache Superset and Apache Druid, but this would be my first contribution as a technical writer.&lt;/p&gt;

&lt;h2 id=&#34;searching-for-the-organization&#34;&gt;Searching for the organization&lt;/h2&gt;

&lt;p&gt;About 40+ organizations were participating in the GSoD. However, there were two which came as the right choice for me in the first instant. The first one was &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Airflow&lt;/a&gt; because I had already used Airflow extensively and also contributed some custom operators inside the forked version of my previous company.&lt;/p&gt;

&lt;p&gt;The second one was &lt;a href=&#34;http://cassandra.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Cassandra&lt;/a&gt;, on which I also had worked extensively but hadn’t done any code or doc changes.&lt;/p&gt;

&lt;p&gt;Considering the total experience, I decided to go with the Airflow.&lt;/p&gt;

&lt;h2 id=&#34;project-selection&#34;&gt;Project selection&lt;/h2&gt;

&lt;p&gt;After selecting the org, the next step was to choose the project. Again, my previous experience played a role here, and I ended up picking the &lt;strong&gt;How to create a workflow&lt;/strong&gt; . The aim of the project was to write documentation which will help users in creating complex as well as custom DAGs.&lt;br /&gt;
The final deliverables were a bit different, though. More on that later.&lt;/p&gt;

&lt;p&gt;After submitting my application, I got involved in my job until one day, I saw a mail from google confirming my selection as a Technical Writer for the project.&lt;/p&gt;

&lt;h2 id=&#34;community-bonding&#34;&gt;Community Bonding&lt;/h2&gt;

&lt;p&gt;Getting selected is just a beginning.  I got the invite to the Airflow slack channel where most of the discussions happened.
My mentor was &lt;a href=&#34;https://github.com/ashb&#34; target=&#34;_blank&#34;&gt;Ash-Berlin Taylor&lt;/a&gt; from Apache Airflow. I started talking to my mentor to get a general sense of what deliverables were expected. The deliverables were documented in &lt;a href=&#34;https://cwiki.apache.org/confluence/display/AIRFLOW/Season+of+Docs+2019&#34; target=&#34;_blank&#34;&gt;confluence&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A page for how to create a DAG that also includes:

&lt;ul&gt;
&lt;li&gt;Revamping the page related to scheduling a DAG&lt;/li&gt;
&lt;li&gt;Adding tips for specific DAG conditions, such as rerunning a failed task&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A page for developing custom operators that includes:

&lt;ul&gt;
&lt;li&gt;Describing mechanisms that are important when creating an operator, such as template fields, UI color, hooks, connection, etc.&lt;/li&gt;
&lt;li&gt;Describing the responsibility between the operator and the hook&lt;/li&gt;
&lt;li&gt;Considerations for dealing with shared resources (such as connections and hooks)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A page that describes how to define the relationships between tasks. The page should include information about:

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt; &amp;gt;&amp;gt; &amp;lt;&amp;lt; &lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;set upstream / set downstream&lt;/li&gt;
&lt;li&gt;helpers method ex. chain&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A page that describes the communication between tasks that also includes:

&lt;ul&gt;
&lt;li&gt;Revamping the page related to macros and XCOM&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My mentor set the expectation early on that the deliverables were sort of like guidelines and not strict rules.
If I wanted to, I could choose to work on something else related to the project also, which was not under deliverables.
After connecting with the mentor, I started engaging with the overall Airflow community. The people in the community were helpful, especially &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil Bregula&lt;/a&gt;. Kamil helped me in getting started with the guidelines to follow while writing the documentation for Airflow.&lt;/p&gt;

&lt;h2 id=&#34;doc-development&#34;&gt;Doc Development&lt;/h2&gt;

&lt;p&gt;I picked DAG run as my first deliverable. I chose this topic as some parts of it were already documented but needed some additional text.
I splitter the existing Scheduling &amp;amp; Triggers page into two new pages.
1. Schedulers
2. DAG Runs&lt;/p&gt;

&lt;p&gt;Most of the details unrelated to schedulers were moved to DAG runs page, and then missing points such as how to re-run a task or DAG were added.
Once I was satisfied with my version, I asked my mentor and Kamil to review it. For the first version, I shared the text in the Google docs file in which the reviewers added comments.
However, the document started getting messy, and it became difficult to track the changes. The time had come now to raise a proper Pull Request.&lt;/p&gt;

&lt;p&gt;This was the time when I faced my first challenge. The documentation of Apache Airflow is written using RST(reStructuredText) syntax, with which I was entirely unfamiliar. I had mostly worked in Markdown.
I spent the next couple of days understanding the syntax. Fortunately, it was quite easy to get acquainted.
I raised the &lt;a href=&#34;https://github.com/apache/airflow/pull/6295&#34; target=&#34;_blank&#34;&gt;Pull Request&lt;/a&gt; and waited for the comments. Finally, after a few days when I saw the comments, they were mostly related to two things - grammar and formatting. There were also comments related to what I had missed or misinterpreted.&lt;/p&gt;

&lt;h3 id=&#34;using-correct-grammar&#34;&gt;Using correct grammar&lt;/h3&gt;

&lt;p&gt;After discussing with Kamil, I decided to follow &lt;a href=&#34;https://developers.google.com/style/&#34; target=&#34;_blank&#34;&gt;Google’s Developer Documentation Guidelines&lt;/a&gt;.  These guidelines contain almost everything you’ll need to consider while writing good documentation, such as always to use active voice.
Secondly, I installed the Grammarly app. After writing a doc, I used to put it in Grammarly to check for errors. Then I corrected the errors, made some more changes, and then again pushed it to Grammarly. This was an iterative process until I arrived with a version of the doc, which was grammatically correct but not seemed to have been written by an AI.&lt;/p&gt;

&lt;h3 id=&#34;formatting&#34;&gt;Formatting&lt;/h3&gt;

&lt;p&gt;Formatting involves writing notes and tips, marking the airflow components correctly in the text, and making sure a user who is skimming through the docs doesn’t miss the critical text.
This required a bit of trial and error. I studied the current pattern in Airflow docs and made changes, pushed commits, incorporated new review comments, and then so on.&lt;/p&gt;

&lt;p&gt;In the end, all the reviewers approved the PR, but it was not merged until two months later. This was because we doubted if some more pages, such as &lt;strong&gt;Concepts&lt;/strong&gt;, should also be split up, resulting in a better-structured document. In the end, we decided to delay it until we discussed it with the broader community.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/apache/airflow/pull/6348&#34; target=&#34;_blank&#34;&gt;second PR&lt;/a&gt; was a completely new document. It was related to How to create your custom operator. For this, since now I was familiar with most of the syntax, I directly raised the PR without going via Google docs. I received a lot of comments again, but this time they were more related to what I had written rather than how I had written it.
e.g., Describing in detail how to use &lt;strong&gt;template fields&lt;/strong&gt; and clean up my code examples. The fewer grammatical &amp;amp; formatting error comments showed I had made progress.
The PR was accepted within two weeks and gave me a huge confidence boost.&lt;/p&gt;

&lt;p&gt;After my second PR, I was in a bit of a deadlock. My last remaining deliverable was related to &lt;strong&gt;Macros&lt;/strong&gt;, but the scope wasn’t clear. I talked to my mentor, and he told me he didn’t mind if I can go off-track to work on something else while the community figured out what changes were needed.
We discussed a lot of ideas. In the end, I decided to go with the Best Practices guide inspired by my mentors’ &lt;a href=&#34;https://drive.google.com/file/d/1E4zle8-fv5S1rrlcNUzjiEV19OMYvwoY/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;talk on Apache Airflow &lt;/a&gt;in a meetup. Having faced challenges while running Airflow in production myself, I was highly motivated to write something like this so that other developers don’t suffer.
The first draft was ready within two weeks. I called it &lt;strong&gt;Running Airflow in Production&lt;/strong&gt;. However, after adding a few more pieces to the document, I realized it was better to call it &lt;strong&gt;Best Practices&lt;/strong&gt; guide, which most of the open-source projects contained.&lt;/p&gt;

&lt;p&gt;People were enthusiastic about this &lt;a href=&#34;https://github.com/apache/airflow/pull/6515&#34; target=&#34;_blank&#34;&gt;pull request&lt;/a&gt; since a lot of them faced the challenges described in the doc. I had hit the nail on the head. After some deliberation over the next 1-2 weeks, my PR got accepted.&lt;/p&gt;

&lt;p&gt;I then returned to my first PR and started making some changes related to the new review comments.  After this, I discussed with my mentor about specific elements that were bugging him, such as getting people to understand how the schedule interval works in as few words as possible.
After a lot of trial and error, we arrived at a version with which both of us could make peace.&lt;/p&gt;

&lt;h2 id=&#34;final-evaluation&#34;&gt;Final Evaluation&lt;/h2&gt;

&lt;p&gt;On 12th September, I received mail from Google about the successful completion of the project. This meant my mentor liked my work. The Airflow community also appreciated the contributions.&lt;/p&gt;

&lt;p&gt;My documents were finally published on Airflow website -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.readthedocs.io/en/latest/dag-run.html&#34; target=&#34;_blank&#34;&gt;DAG Runs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.readthedocs.io/en/latest/scheduler.html&#34; target=&#34;_blank&#34;&gt;Scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.readthedocs.io/en/latest/howto/custom-operator.html&#34; target=&#34;_blank&#34;&gt;Creating a custom operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://airflow.readthedocs.io/en/latest/best-practices.html&#34; target=&#34;_blank&#34;&gt;Best Practices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also started getting invited in the PR reviews of other developers. I am looking forward to more contributions to the project in the coming year.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Airflow Survey 2019</title>
      <link>/blog/airflow-survey/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-survey/</guid>
      <description>
        
        
        

&lt;h1 id=&#34;apache-airflow-survey-2019&#34;&gt;Apache Airflow Survey 2019&lt;/h1&gt;

&lt;p&gt;Apache Airflow is &lt;a href=&#34;https://www.astronomer.io/blog/why-airflow/&#34; target=&#34;_blank&#34;&gt;growing faster than ever&lt;/a&gt;.
Thus, receiving and adjusting to our users’ feedback is a must. We created
&lt;a href=&#34;https://forms.gle/XAzR1pQBZiftvPQM7&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; and we got &lt;strong&gt;308&lt;/strong&gt; responses.
Let’s see who Airflow users are, how they play with it, and what they miss.&lt;/p&gt;

&lt;h1 id=&#34;overview-of-the-user&#34;&gt;Overview of the user&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;What best describes your current occupation?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data Engineer&lt;/td&gt;
&lt;td&gt;194&lt;/td&gt;
&lt;td&gt;62.99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Developer&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;11.04%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Architect&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;7.47%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Data Scientist&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;6.17%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Data Analyst&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DevOps&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;IT Administrator&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Machine Learning Engineer&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Manager&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Operations&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Chief Data Officer&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Engineering Manager&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Intern&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Product owner&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Quant&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;In your day to day job, what do you use Airflow for?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data processing (ETL)&lt;/td&gt;
&lt;td&gt;298&lt;/td&gt;
&lt;td&gt;96.75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Artificial Intelligence and Machine Learning Pipelines&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;29.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Automating DevOps operations&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;20.78%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;According to the survey, most of the Airflow users are the “data” people. Moreover,
28.57% uses Airflow to both ETL and ML pipelines meaning that those two fields
are somehow connected. Only five respondents use Airflow for DevOps operations only,
That means that other 59 people who use Airflow for DevOps stuff use it also for
ETL / ML  purposes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How many active DAGs do you have in your largest Airflow instance?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0-20&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;37.34%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;21-40&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;21.10%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;41-60&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;14.29%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;61-100&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;101-200&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;201-300&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;301-999&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1000+&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The majority of users do not exceed 100 active DAGs per Airflow instance. However,
as we can see there are users who exceed thousands of DAGs with a maximum number 5000.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the maximum number of tasks that you have used in one DAG?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0-10&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;19.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11-20&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;19.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;21-30&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;10.06%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;31-40&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;6.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;41-50&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;8.44%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;51-100&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;11.69%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;101-200&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;201-500&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;6.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;501+&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;11.54%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The given maximum number of tasks in a single DAG was 10 000 (!). The number of tasks
depends on the purposes of a DAG, so it’s rather hard to say if users have “simple”
or “complicated” workflows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When onboarding new members to Airflow, what is the biggest problem?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No guide on best practises on developing DAGs&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;51.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Small number of tutorials on different aspects of using Airflow&lt;/td&gt;
&lt;td&gt;57&lt;/td&gt;
&lt;td&gt;18.51%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Documentation is not clear enough&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;13.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Small number of blogs regarding Airflow&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;13.96%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This is an important result. Using Airflow is all about writing and scheduling DAGs.
No guide or any other complete resource on best practices for developing Dags is a big
problem. Diving deep in the “other” answers, we can find that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Airflow’s “magic” (scheduler, executors, schedule times) is hard to understand&lt;/li&gt;
&lt;li&gt;DAG testing is not easy to do and to explain&lt;/li&gt;
&lt;li&gt;Airflow UI needs some love.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How likely are you to recommend Apache Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Very Likely&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;45.45%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Likely&lt;/td&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;40.26%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Neutral&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;10.71%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Unlikely&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Very unlikely&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.97%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This means that more than 85% of people who use Airflow like it. It seems Airflow does
its job nicely. However, we have to remember that this survey is likely biased - it’s
more likely that you respond to the survey if you like the tool you use. Should we
focus then on those 11 people who did not like Airflow? It’s a good question.&lt;/p&gt;

&lt;h2 id=&#34;airflow-usage&#34;&gt;Airflow usage&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Which interface(s) of Airflow do you use as part of your current role?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface&lt;/td&gt;
&lt;td&gt;297&lt;/td&gt;
&lt;td&gt;96.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CLI&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;td&gt;40.91%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface, CLI&lt;/td&gt;
&lt;td&gt;117&lt;/td&gt;
&lt;td&gt;37.99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;API&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;19.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface, CLI, API&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;10.39%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Custom (own created) Airflow Graphical User Interface&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;8.12%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It’s visible that usage of CLI goes in pair with using Airflow web UI. Our
survey included some UX related questions to allow us to understand how users
use Airflow webserver.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do you use the Graphical User Interface for?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do you use CLI for?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Airflow, which UI view(s) are important for you?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here we see that the majority uses Web UI mostly for monitoring purposes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Monitoring DAGs&lt;/li&gt;
&lt;li&gt;Accessing logs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting result is that many people seem not to use backfilling as
there’s no other way than to do it by CLI.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What executor type do you use?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Celery&lt;/td&gt;
&lt;td&gt;138&lt;/td&gt;
&lt;td&gt;44.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Local&lt;/td&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;td&gt;27.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Kubernetes&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;16.88%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sequential&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;7.14%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;3.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The other option mostly consisted of information that someone uses a few types or is
migrating from one executor to another. What can be observed is an increase in usage
of Local and Kubernetes executors when compared to results from an earlier &lt;a href=&#34;https://ash.berlintaylor.com/writings/2019/02/airflow-user-survey-2019/&#34; target=&#34;_blank&#34;&gt;survey done
by Ash&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you use Kubernetes-based deployments for Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No - we do not plan to use Kubernetes near term&lt;/td&gt;
&lt;td&gt;88&lt;/td&gt;
&lt;td&gt;28.57%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes - setup on our own via Helm Chart or similar&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;21.10%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Not yet - but we use Kubernetes in our organization and we could move&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;19.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes - via managed service in the cloud (Composer / Astronomer etc.)&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;td&gt;14.61%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Not yet - but we plan to deploy Kubernetes in our organization soon&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;13.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The most interesting thing is that there’s nearly 30% of users who do not use Kubernetes,
and they are not going to move. This means we should keep other deployment options in
mind when working on Airflow 2.0. On the other hand, almost 70% of the users already
use Kubernetes, or it’s a viable option for them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you combine multiple DAGs?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No, I don&amp;rsquo;t combine multiple DAGs&lt;/td&gt;
&lt;td&gt;127&lt;/td&gt;
&lt;td&gt;41.23%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes, through SubDAG&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;23.70%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes, by triggering another DAG&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;23.38%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;11.69%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the other category, 9 people explicitly mentioned using &lt;code&gt;ExternalTaskSensor&lt;/code&gt;,
and I think it could be treated as running subDAGs by triggering other DAGs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you use Airflow Plugins? If yes, what do you use it for?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Adding new operators/sensors and hooks&lt;/td&gt;
&lt;td&gt;187&lt;/td&gt;
&lt;td&gt;60.71%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I don&amp;rsquo;t use Airflow plugins&lt;/td&gt;
&lt;td&gt;109&lt;/td&gt;
&lt;td&gt;35.39%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding AppBuilder views &amp;amp; menu items&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;10.06%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding new executor&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;5.84%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding OperatorExtraLinks&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The high percentage - 60%  for “Adding new operators/sensors and hooks” is quite a
surprising result for some of us - especially that you do not actually need to use the
plugin mechanism to add any of those. Those are standard python objects, and you can
simply drop your hooks/operators/sensors code to &lt;code&gt;PYTHONPATH&lt;/code&gt; environment variable and
they will work. It seems that this may be a result of a lack of best practices guide.&lt;/p&gt;

&lt;p&gt;Plugins are more useful for adding views and menu items - yet only 10%.
OperatorExtraLinks are even more useful (though relatively new) feature, so it’s not
entirely surprising they are hardly used.&lt;/p&gt;

&lt;p&gt;It was also kind of surprising that someone at all uses plugins to use their own
executors. We considered removing that option recently - but now we have to rethink
our approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What metrics do you use to monitor Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There were a lot of different responses. Some use Prometheus and other services,
others do not use any monitoring. One of the interesting responses linked to this
solution for &lt;a href=&#34;https://github.com/mastak/airflow_operators_metrics&#34; target=&#34;_blank&#34;&gt;airflow_operators_metrics&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;external-services&#34;&gt;External services&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What external services do you use in your Airflow DAGs?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Amazon Web Services&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;51.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Internal company systems&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;48.7%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hadoop / Spark / Flink / Other Apache software&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;38.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;36.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Microsoft Azure&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I do not use external services in my Airflow DAGs&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;5.84%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It’s not surprising that Amazon Web Services is leading the way as they are considered the most mature
cloud provider. Internal system and other Apache products on the next two positions are
quite understandable if we take into account that the majority uses Airflow for ETL processes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What external services do you use in your Airflow DAGs? (Mixed providers)&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs, Amazon Web Services&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;14.29%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Amazon Web Services, Microsoft Azure&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1.62%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs, Microsoft Azure&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This result is not surprising because companies usually prefer to stick with one cloud
provider.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do you integrate with external services?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Using Bash / Python operator&lt;/td&gt;
&lt;td&gt;220&lt;/td&gt;
&lt;td&gt;71.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Using existing, dedicated operators / hooks&lt;/td&gt;
&lt;td&gt;217&lt;/td&gt;
&lt;td&gt;70.45%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Using own, custom operators / hooks&lt;/td&gt;
&lt;td&gt;216&lt;/td&gt;
&lt;td&gt;70.13%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We had some anecdotal evidence that people use more Python/Bash operators than the
dedicated ones - but it looks like all ways of using Airflow to connect to external
services are equally popular.&lt;/p&gt;

&lt;h2 id=&#34;what-can-be-improved&#34;&gt;What can be improved&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;In your opinion, what could be improved in Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scheduler performance&lt;/td&gt;
&lt;td&gt;189&lt;/td&gt;
&lt;td&gt;61.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Web UI&lt;/td&gt;
&lt;td&gt;180&lt;/td&gt;
&lt;td&gt;58.44%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Logging, monitoring and alerting&lt;/td&gt;
&lt;td&gt;145&lt;/td&gt;
&lt;td&gt;47.08%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Examples, how-to, onboarding documentation&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;46.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Technical documentation&lt;/td&gt;
&lt;td&gt;137&lt;/td&gt;
&lt;td&gt;44.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Reliability&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;36.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;REST API&lt;/td&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;31.17%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Authentication and authorization&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;28.9%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;External integration e.g. AWS, GCP, Apache product&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;15.91%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CLI&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;13.31%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I don’t know&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1.62%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The results are rather quite self-explaining. Improved performance of Airflow, better
UI, and more telemetry are desirable. But this should go in pair with improved
documentation and resources about using the Airflow, especially when we
take into account the problem of onboarding new users.&lt;/p&gt;

&lt;p&gt;Another interesting point from that question is that only 16% think that operators
should be extended and improved. This suggests that we should focus on improving
Airflow core instead of adding more and more integrations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What would be the most interesting feature for you?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Production-ready Airflow docker image&lt;/td&gt;
&lt;td&gt;175&lt;/td&gt;
&lt;td&gt;56.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Declarative way of writing DAGs / automated DAGs generation&lt;/td&gt;
&lt;td&gt;155&lt;/td&gt;
&lt;td&gt;50.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Horizontal Autoscaling&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;39.61%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Asynchronous Operators&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;31.49%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Stateless web server&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;26.3%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Knative Executor&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;15.58%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I already have all I need&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Production Docker image wins, and it’s not a surprise. We all know that deploying
Airflow is not a plug and play process, and that’s why the official image is being
worked on by Jarek Potiuk. An unexpected result is that half of the users would like to
have a declarative way of creating DAGs. That seems to be something that is “against Airflow”
as we always emphasize the possibility of writing workflows in pure python. Stories
about DAG generators are not new and confirm that there’s a need for a way to
declare DAGs.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;If you think I missed something and you want to look for insights on your own the data is available
for you here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Original data: &lt;a href=&#34;https://storage.googleapis.com/airflow-survey/survey.csv&#34; target=&#34;_blank&#34;&gt;https://storage.googleapis.com/airflow-survey/survey.csv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Processed: &lt;a href=&#34;https://storage.googleapis.com/airflow-survey/airflow_survey_processed.csv&#34; target=&#34;_blank&#34;&gt;https://storage.googleapis.com/airflow-survey/airflow_survey_processed.csv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The processed data includes multi-choice options one-hot encoded. If you find any interesting
insight, please update the article (&lt;a href=&#34;https://github.com/apache/airflow-site/blob/master/CONTRIBUTE.md&#34; target=&#34;_blank&#34;&gt;make PR&lt;/a&gt;
to Airflow site).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: New Airflow website</title>
      <link>/blog/announcing-new-website/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/announcing-new-website/</guid>
      <description>
        
        
        &lt;p&gt;The brand &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;new Airflow website&lt;/a&gt; has arrived! Those who have been following the process know that the journey to update &lt;a href=&#34;https://airflow.readthedocs.io/en/1.10.6/&#34; target=&#34;_blank&#34;&gt;the old Airflow website&lt;/a&gt; started at the beginning of the year.
Thanks to sponsorship from the Cloud Composer team at Google that allowed us to
collaborate with &lt;a href=&#34;https://www.polidea.com/&#34; target=&#34;_blank&#34;&gt;Polidea&lt;/a&gt; and with their design studio &lt;a href=&#34;https://utilodesign.com/&#34; target=&#34;_blank&#34;&gt;Utilo&lt;/a&gt;, and deliver an awesome website.&lt;/p&gt;

&lt;p&gt;Documentation of open source projects is key to engaging new contributors in the maintenance,
development, and adoption of software. We want the Apache Airflow community to have
the best possible experience to contribute and use the project. We also took this opportunity to make the project
more accessible, and in doing so, increase its reach.&lt;/p&gt;

&lt;p&gt;In the past three and a half months, we have updated everything: created a more efficient landing page,
enhanced information architecture, and improved UX &amp;amp; UI. Most importantly, the website now has capabilities
to be translated into many languages. This is our effort to foster a more inclusive community around
Apache Airflow, and we look forward to seeing contributions in Spanish, Chinese, Russian, and other languages as well!&lt;/p&gt;

&lt;p&gt;We built our website on Docsy, a platform that is easy to use and contribute to. Follow
&lt;a href=&#34;https://github.com/apache/airflow-site/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;these steps&lt;/a&gt; to set up your environment and
to create your first pull request. You may also use
the new website for your own open source project as a template.
All of our &lt;a href=&#34;https://github.com/apache/airflow-site/tree/master&#34; target=&#34;_blank&#34;&gt;code is open and hosted on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Share your questions, comments, and suggestions with us, to help us improve the website.
We hope that this new design makes finding documentation about Airflow easier,
and that its improved accessibility increases adoption and use of Apache Airflow around the world.&lt;/p&gt;

&lt;p&gt;Happy browsing!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers</title>
      <link>/blog/apache-con-europe-2019-thoughts-and-insights-by-airflow-committers/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/apache-con-europe-2019-thoughts-and-insights-by-airflow-committers/</guid>
      <description>
        
        
        &lt;p&gt;Is it possible to create an organization that delivers tens of projects used by millions, nearly no one is paid for doing their job, and still, it has been fruitfully carrying on for more than 20 years? Apache Software Foundation proves it is possible. For the last two decades, ASF has been crafting a model called the Apache Way—a way of organizing and leading tech open source projects. Due to this approach, which is strongly based on the “community over code” motto, we can enjoy such awesome projects like Apache Spark, Flink, Beam, or Airflow (and many more).&lt;/p&gt;

&lt;p&gt;After this year’s ApacheCon, Polidea’s engineers talked with Committers of Apache projects, such as—Aizhamal Nurmamat kyzy, Felix Uellendall, and Fokko Driesprong—about insights to what makes the ASF such an amazing organization.&lt;/p&gt;

&lt;p&gt;You can read the &lt;a href=&#34;https://higrys.medium.com/apachecon-europe-2019-thoughts-and-insights-by-airflow-committers-9ff5f6938c99&#34; target=&#34;_blank&#34;&gt;insights after the ApacheCon 2019&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Documenting using local development environment</title>
      <link>/blog/documenting-using-local-development-environments/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/documenting-using-local-development-environments/</guid>
      <description>
        
        
        

&lt;h2 id=&#34;documenting-local-development-environment-of-apache-airflow&#34;&gt;Documenting local development environment of Apache Airflow&lt;/h2&gt;

&lt;p&gt;From Sept to November, 2019 I have been participating in a wonderful initiative, &lt;a href=&#34;https://developers.google.com/season-of-docs&#34; target=&#34;_blank&#34;&gt;Google Season of Docs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I had a pleasure to contribute to the Apache Airflow open source project as a technical writer.
My initial assignment was an extension to the GitHub-based Contribution guide.&lt;/p&gt;

&lt;p&gt;From the very first days I have been pretty closely involved into inter-project communications
via emails/slack and had regular 1:1s with my mentor, Jarek Potiuk.&lt;/p&gt;

&lt;p&gt;I got infected with Jarek’s enthusiasm to ease the on-boarding experience for
Airflow contributors. I do share this strategy and did my best to improve the structure,
language and DX. As a result, Jarek and I extended the current contributor’s docs and
ended up with the Contributing guide navigating the users through the project
infrastructure and providing a workflow example based on a real-life use case;
the Testing guide with an overview of a complex testing infrastructure for Apache Airflow;
and two guides dedicated to the Breeze dev environment and local virtual environment
(my initial assignment).&lt;/p&gt;

&lt;p&gt;I’m deeply grateful to my mentor and Airflow developers for their feedback,
patience and help while I was breaking through new challenges
(I’ve never worked on an open source project before),
and for their support of all my ideas! I think a key success factor for any contributor
is a responsive, supportive and motivated team, and I was lucky to join such
a team for 3 months.&lt;/p&gt;

&lt;p&gt;Documents I worked on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze development environment documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/blob/master/LOCAL_VIRTUALENV.rst&#34; target=&#34;_blank&#34;&gt;Local virtualenv environment documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst&#34; target=&#34;_blank&#34;&gt;Contributing guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow/blob/master/TESTING.rst&#34; target=&#34;_blank&#34;&gt;Testing guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: It&#39;s a &#34;Breeze&#34; to develop Apache Airflow</title>
      <link>/blog/its-a-breeze-to-develop-apache-airflow/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/its-a-breeze-to-develop-apache-airflow/</guid>
      <description>
        
        
        

&lt;h2 id=&#34;the-story-behind-the-airflow-breeze-tool&#34;&gt;The story behind the Airflow Breeze tool&lt;/h2&gt;

&lt;p&gt;Initially, we started contributing to this fantastic open-source project [Apache Airflow] with a team of three which then grew to five. When we kicked it off a year ago, I realized pretty soon where the biggest bottlenecks and areas for improvement in terms of productivity were. Even with the help of our client, who provided us with a “homegrown” development environment it took us literally days to set it up and learn some basics.&lt;/p&gt;

&lt;p&gt;That is how the journey to increased productivity in Apache Airflow began. The result? The Airflow Breeze open-source tool. Jarek Potiuk, an Airflow Committer, will tell you all about it.&lt;/p&gt;

&lt;p&gt;You can learn &lt;a href=&#34;https://higrys.medium.com/its-a-breeze-to-develop-apache-airflow-bf306d3e3505&#34; target=&#34;_blank&#34;&gt;how and why it’s a &amp;ldquo;Breeze&amp;rdquo; to Develop Apache Airflow&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
