<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Airflow – Community</title>
    <link>/blog/tags/community/</link>
    <description>Recent content in Community on Apache Airflow</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 30 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="/blog/tags/community/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Blog: Journey with Airflow as an Outreachy Intern</title>
      <link>/blog/experience-with-airflow-as-an-outreachy-intern/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/experience-with-airflow-as-an-outreachy-intern/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;a href=&#34;https://www.outreachy.org/&#34; target=&#34;_blank&#34;&gt;Outreachy&lt;/a&gt; is a program which organises three months paid internships with FOSS
projects for people who are typically underrepresented in those projects.&lt;/p&gt;

&lt;h3 id=&#34;contribution-period&#34;&gt;Contribution Period&lt;/h3&gt;

&lt;p&gt;The first thing I had to do was choose a project under an organisation. After going through all the projects
I chose “Extending the REST API of Apache Airflow”, because I had a good idea of what  REST API(s) are, so I
thought it would be easier to get started with the contributions. The next step was to set up Airflow’s dev
environment which thanks to &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze&lt;/a&gt;, was a breeze.
Since I had never contributed to FOSS before so this part was overwhelming but there were plenty of issues
labelled “good first issues” with detailed descriptions and some even had code snippets so luckily that nudged
me in the right direction. These things about Airflow and the positive vibes from the community were the reasons
why I chose to stick with Airflow as my Outreachy project.&lt;/p&gt;

&lt;h3 id=&#34;internship-period&#34;&gt;Internship Period&lt;/h3&gt;

&lt;p&gt;My first PR was followed by many new experiences one of them being that I introduced a
&lt;a href=&#34;https://github.com/apache/airflow/pull/7680#issuecomment-619763051&#34; target=&#34;_blank&#34;&gt;bug&lt;/a&gt; in it;).
But nonetheless it made me familiar with the feedback loop and the feedback on my subsequent
&lt;a href=&#34;https://github.com/apache/airflow/pulls?q=is%3Apr+author%3AOmairK+&#34; target=&#34;_blank&#34;&gt;PRs&lt;/a&gt; was the focal point of the overall
learning experience I went through, which boosted my confidence to contribute more and move out of my comfort zone.
I wanted to learn more about the things that happen under the Airflow’s hood so I started filtering out recent PRs
dealing with different components and I would go through the code changes along with discussion that would help me
get a better understanding of the whole workflow. &lt;a href=&#34;https://lists.apache.org/list.html?dev@airflow.apache.org&#34; target=&#34;_blank&#34;&gt;Airflow’s mailing list&lt;/a&gt;
was also a great source of knowledge.&lt;/p&gt;

&lt;p&gt;The API related PRs that I worked on helped me with some of the important concepts like:&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&#34;https://github.com/apache/airflow/pull/9329&#34; target=&#34;_blank&#34;&gt;Pool CRUD endpoints&lt;/a&gt; where pools limit the execution parallelism.&lt;/p&gt;

&lt;p&gt;2) &lt;a href=&#34;https://github.com/apache/airflow/pull/9597&#34; target=&#34;_blank&#34;&gt;Tasks&lt;/a&gt; determine the actual work that has to be carried out.&lt;/p&gt;

&lt;p&gt;3) &lt;a href=&#34;https://github.com/apache/airflow/pull/9473&#34; target=&#34;_blank&#34;&gt;DAG&lt;/a&gt; which represents the structure for a collection
  of tasks. It keeps track of tasks, their dependencies and the sequence in which they have to run.&lt;/p&gt;

&lt;p&gt;4) &lt;a href=&#34;https://github.com/apache/airflow/pull/9473&#34; target=&#34;_blank&#34;&gt;Dag Runs&lt;/a&gt; that are the instantiation of DAG(s) in time.&lt;/p&gt;

&lt;p&gt;Through actively and passively participating in discussions I learnt that even if there is a difference of opinion
one could always learn from the different approaches, and &lt;a href=&#34;https://github.com/apache/airflow/pull/8721&#34; target=&#34;_blank&#34;&gt;this PR&lt;/a&gt; with
more than 300+ comments is the proof of it. I also started reviewing small PRs which gave me the amazing opportunity
to interact with new people. Throughout my internship I learnt a lot about different frameworks and technologies
but the biggest takeaway for me was that a code is read more often than it&amp;rsquo;s written, and I started writing code with
that in mind.&lt;/p&gt;

&lt;h3 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h3&gt;

&lt;p&gt;So with my project of extending Airflow’s REST API as well as the Outreachy internship coming to an end I would like
to thank my mentors &lt;a href=&#34;https://github.com/potiuk&#34; target=&#34;_blank&#34;&gt;Jarek Potiuk&lt;/a&gt;, &lt;a href=&#34;https://github.com/kaxil&#34; target=&#34;_blank&#34;&gt;Kaxil Naik&lt;/a&gt; and
&lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil Breguła&lt;/a&gt; for the patience and the time they invested in mentoring me and
the Airflow community for making me feel so welcomed. I plan to stick around and contribute to give back to the
community that has been made my summer, one to remember.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Apache Airflow For Newcomers</title>
      <link>/blog/apache-airflow-for-newcomers/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/apache-airflow-for-newcomers/</guid>
      <description>
        
        
        

&lt;p&gt;Apache Airflow is a platform to programmatically author, schedule, and monitor workflows.
A workflow is a sequence of tasks that processes a set of data. You can think of workflow as the
path that describes how tasks go from being undone to done. Scheduling, on the other hand, is the
process of planning, controlling, and optimizing when a particular task should be done.&lt;/p&gt;

&lt;h3 id=&#34;authoring-workflow-in-apache-airflow&#34;&gt;Authoring Workflow in Apache Airflow.&lt;/h3&gt;

&lt;p&gt;Airflow makes it easy to author workflows using python scripts. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34;&gt;Directed Acyclic Graph&lt;/a&gt;
(DAG) represents a workflow in Airflow. It is a collection of tasks in a way that shows each task&amp;rsquo;s
relationships and dependencies. You can have as many DAGs as you want, and Airflow will execute
them according to the task&amp;rsquo;s relationships and dependencies. If task B depends on the successful
execution of another task A, it means Airflow will run task A and only run task B after task A.
This dependency is very easy to express in Airflow. For example, the above scenario is expressed as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;task_A &amp;gt;&amp;gt; task_B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also equivalent to&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;task_A.set_downstream(task_B)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Simple_dag.png&#34; alt=&#34;Simple Dag&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That helps Airflow to know that it needs to execute task A before task B. Tasks can have far more complex
relationships to each other than expressed above and Airflow figures out how and when to execute the tasks following
their relationships and dependencies.
&lt;img src=&#34;semicomplex.png&#34; alt=&#34;Complex Dag&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before we discuss the architecture of Airflow that makes scheduling, executing, and monitoring of
workflow an easy thing, let us discuss the &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze environment&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;breeze-environment&#34;&gt;Breeze Environment&lt;/h3&gt;

&lt;p&gt;The breeze environment is the development environment for Airflow where you can run tests, build images,
build documentations and so many other things. There are excellent
&lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;documentation and video&lt;/a&gt; on Breeze environment.
Please check them out. You enter the Breeze environment by running the &lt;code&gt;./breeze&lt;/code&gt; script. You can run all
the commands mentioned here in the Breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;scheduler&#34;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;The scheduler is the component that monitors DAGs and triggers those tasks whose dependencies have
been met. It watches over the DAG folder, checking the tasks in each DAG and triggers them once they
are ready. It accomplishes this by spawning a process that runs periodically(every minute or so)
reading the metadata database to check the status of each task and decides what needs to be done.
The metadata database is where the status of all tasks are recorded. The status can be one of running,
 success, failed, etc.&lt;/p&gt;

&lt;p&gt;A task is said to be ready when its dependencies have been met. The dependencies include all the data
necessary for the task to be executed. It should be noted that the scheduler won&amp;rsquo;t trigger your tasks until
the period it covers has ended. If a task&amp;rsquo;s &lt;code&gt;schedule_interval&lt;/code&gt; is &lt;code&gt;@daily&lt;/code&gt;, the scheduler triggers the task
at the end of the day and not at the beginning. This is to ensure that the necessary data needed for the tasks
are ready. It is also possible to trigger tasks manually on the UI.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;https://github.com/apache/airflow/blob/master/BREEZE.rst&#34; target=&#34;_blank&#34;&gt;Breeze environment&lt;/a&gt;, the scheduler is started by running the command &lt;code&gt;airflow scheduler&lt;/code&gt;. It uses
the configured production environment. The configuration can be specified in &lt;code&gt;airflow.cfg&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;executor&#34;&gt;Executor&lt;/h3&gt;

&lt;p&gt;Executors are responsible for running tasks. They work with the scheduler to get information about
what resources are needed to run a task as the task is queued.&lt;/p&gt;

&lt;p&gt;By default, Airflow uses the &lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/sequential.html#sequential-executor&#34; target=&#34;_blank&#34;&gt;SequentialExecutor&lt;/a&gt;.
 However, this executor is limited and it is the only executor that can be used with SQLite.&lt;/p&gt;

&lt;p&gt;There are many other &lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/index.html&#34; target=&#34;_blank&#34;&gt;executors&lt;/a&gt;,
 the difference is on the resources they have and how they choose to use the resources. The available executors
 are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sequential Executor&lt;/li&gt;
&lt;li&gt;Debug Executor&lt;/li&gt;
&lt;li&gt;Local Executor&lt;/li&gt;
&lt;li&gt;Dask Executor&lt;/li&gt;
&lt;li&gt;Celery Executor&lt;/li&gt;
&lt;li&gt;Kubernetes Executor&lt;/li&gt;
&lt;li&gt;Scaling Out with Mesos (community contributed)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CeleryExecutor is a better executor compared to the SequentialExecutor. The CeleryExecutor uses several
workers to execute a job in a distributed way. If a worker node is ever down, the CeleryExecutor assign its
task to another worker node. This ensures high availability.&lt;/p&gt;

&lt;p&gt;The CeleryExecutor works closely with the scheduler which adds a message to the queue and the Celery broker
which delivers the message to a Celery worker to execute.
You can find more information about the CeleryExecutor and how to configure it at the
&lt;a href=&#34;https://airflow.apache.org/docs/stable/executor/celery.html#celery-executor&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;webserver&#34;&gt;Webserver&lt;/h3&gt;

&lt;p&gt;The webserver is the web interface (UI) for Airflow. The UI is feature-rich. It makes it easy to
monitor and troubleshoot DAGs and Tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;airflow-ui.png&#34; alt=&#34;airflow UI&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are many actions you can perform on the UI. You can trigger a task, monitor the execution
including the duration of the task. The UI makes it possible to view the task&amp;rsquo;s dependencies in a
tree view and graph view. You can view task logs in the UI.&lt;/p&gt;

&lt;p&gt;The web UI is started with the command &lt;code&gt;airflow webserver&lt;/code&gt; in the breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;backend&#34;&gt;Backend&lt;/h3&gt;

&lt;p&gt;By default, Airflow uses the SQLite backend for storing the configuration information, DAG states,
and much other useful information. This should not be used in production as SQLite can cause a data
loss.&lt;/p&gt;

&lt;p&gt;You can use PostgreSQL or MySQL as a backend for airflow. It is easy to change to PostgreSQL or MySQL.&lt;/p&gt;

&lt;p&gt;The command &lt;code&gt;./breeze --backend mysql&lt;/code&gt; selects MySQL as the backend when starting the breeze environment.&lt;/p&gt;

&lt;h3 id=&#34;operators&#34;&gt;Operators&lt;/h3&gt;

&lt;p&gt;Operators determine what gets done by a task. Airflow has a lot of builtin Operators. Each operator
does a specific task. There&amp;rsquo;s a BashOperator that executes a bash command, the PythonOperator which
calls a python function, AwsBatchOperator which executes a job on AWS Batch and &lt;a href=&#34;https://airflow.apache.org/docs/stable/concepts.html#operators&#34; target=&#34;_blank&#34;&gt;many more&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;sensors&#34;&gt;Sensors&lt;/h4&gt;

&lt;p&gt;Sensors can be described as special operators that are used to monitor a long-running task.
Just like Operators, there are many predefined sensors in Airflow. These includes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AthenaSensor: Asks for the state of the Query until it reaches a failure state or success state.&lt;/li&gt;
&lt;li&gt;AzureCosmosDocumentSensor: Checks for the existence of a document which matches the given query in CosmosDB&lt;/li&gt;
&lt;li&gt;GoogleCloudStorageObjectSensor:  Checks for the existence of a file in Google Cloud Storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A list of most of the available sensors can be found in this &lt;a href=&#34;https://airflow.apache.org/docs/stable/_api/airflow/contrib/sensors/index.html?highlight=sensors#module-airflow.contrib.sensors&#34; target=&#34;_blank&#34;&gt;module&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contributing-to-airflow&#34;&gt;Contributing to Airflow&lt;/h3&gt;

&lt;p&gt;Airflow is an open source project, everyone is welcome to contribute. It is easy to get started thanks
to the excellent &lt;a href=&#34;https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst&#34; target=&#34;_blank&#34;&gt;documentation on how to get started&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I joined the community about 12 weeks ago through the &lt;a href=&#34;https://www.outreachy.org/&#34; target=&#34;_blank&#34;&gt;Outreachy Program&lt;/a&gt; and have
completed about &lt;a href=&#34;https://github.com/apache/airflow/pulls/ephraimbuddy&#34; target=&#34;_blank&#34;&gt;40 PRs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It has been an amazing experience! Thanks to my mentors &lt;a href=&#34;https://github.com/potiuk&#34; target=&#34;_blank&#34;&gt;Jarek&lt;/a&gt; and
&lt;a href=&#34;https://github.com/kaxil&#34; target=&#34;_blank&#34;&gt;Kaxil&lt;/a&gt;, and the community members especially &lt;a href=&#34;https://github.com/mik-laj&#34; target=&#34;_blank&#34;&gt;Kamil&lt;/a&gt;
and &lt;a href=&#34;https://github.com/turbaszek&#34; target=&#34;_blank&#34;&gt;Tomek&lt;/a&gt; for all their support. I&amp;rsquo;m grateful!&lt;/p&gt;

&lt;p&gt;Thank you so much, &lt;a href=&#34;https://github.com/leahecole&#34; target=&#34;_blank&#34;&gt;Leah E. Cole&lt;/a&gt;, for your wonderful reviews.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Airflow Survey 2019</title>
      <link>/blog/airflow-survey/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/airflow-survey/</guid>
      <description>
        
        
        

&lt;h1 id=&#34;apache-airflow-survey-2019&#34;&gt;Apache Airflow Survey 2019&lt;/h1&gt;

&lt;p&gt;Apache Airflow is &lt;a href=&#34;https://www.astronomer.io/blog/why-airflow/&#34; target=&#34;_blank&#34;&gt;growing faster than ever&lt;/a&gt;.
Thus, receiving and adjusting to our users’ feedback is a must. We created
&lt;a href=&#34;https://forms.gle/XAzR1pQBZiftvPQM7&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; and we got &lt;strong&gt;308&lt;/strong&gt; responses.
Let’s see who Airflow users are, how they play with it, and what they miss.&lt;/p&gt;

&lt;h1 id=&#34;overview-of-the-user&#34;&gt;Overview of the user&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;What best describes your current occupation?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data Engineer&lt;/td&gt;
&lt;td&gt;194&lt;/td&gt;
&lt;td&gt;62.99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Developer&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;11.04%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Architect&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;7.47%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Data Scientist&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;6.17%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Data Analyst&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DevOps&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;IT Administrator&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Machine Learning Engineer&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Manager&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Operations&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.65%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Chief Data Officer&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Engineering Manager&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Intern&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Product owner&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Quant&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.32%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;In your day to day job, what do you use Airflow for?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data processing (ETL)&lt;/td&gt;
&lt;td&gt;298&lt;/td&gt;
&lt;td&gt;96.75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Artificial Intelligence and Machine Learning Pipelines&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;29.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Automating DevOps operations&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;20.78%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;According to the survey, most of the Airflow users are the “data” people. Moreover,
28.57% uses Airflow to both ETL and ML pipelines meaning that those two fields
are somehow connected. Only five respondents use Airflow for DevOps operations only,
That means that other 59 people who use Airflow for DevOps stuff use it also for
ETL / ML  purposes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How many active DAGs do you have in your largest Airflow instance?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0-20&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;37.34%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;21-40&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;21.10%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;41-60&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;14.29%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;61-100&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;101-200&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;201-300&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;301-999&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1000+&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The majority of users do not exceed 100 active DAGs per Airflow instance. However,
as we can see there are users who exceed thousands of DAGs with a maximum number 5000.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the maximum number of tasks that you have used in one DAG?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0-10&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;19.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11-20&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;19.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;21-30&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;10.06%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;31-40&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;6.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;41-50&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;8.44%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;51-100&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;11.69%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;101-200&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;201-500&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;6.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;501+&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;11.54%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The given maximum number of tasks in a single DAG was 10 000 (!). The number of tasks
depends on the purposes of a DAG, so it’s rather hard to say if users have “simple”
or “complicated” workflows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When onboarding new members to Airflow, what is the biggest problem?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No guide on best practises on developing DAGs&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;51.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Small number of tutorials on different aspects of using Airflow&lt;/td&gt;
&lt;td&gt;57&lt;/td&gt;
&lt;td&gt;18.51%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Documentation is not clear enough&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;13.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Small number of blogs regarding Airflow&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;13.96%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This is an important result. Using Airflow is all about writing and scheduling DAGs.
No guide or any other complete resource on best practices for developing Dags is a big
problem. Diving deep in the “other” answers, we can find that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Airflow’s “magic” (scheduler, executors, schedule times) is hard to understand&lt;/li&gt;
&lt;li&gt;DAG testing is not easy to do and to explain&lt;/li&gt;
&lt;li&gt;Airflow UI needs some love.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How likely are you to recommend Apache Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Very Likely&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;45.45%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Likely&lt;/td&gt;
&lt;td&gt;124&lt;/td&gt;
&lt;td&gt;40.26%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Neutral&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;10.71%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Unlikely&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;2.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Very unlikely&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.97%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This means that more than 85% of people who use Airflow like it. It seems Airflow does
its job nicely. However, we have to remember that this survey is likely biased - it’s
more likely that you respond to the survey if you like the tool you use. Should we
focus then on those 11 people who did not like Airflow? It’s a good question.&lt;/p&gt;

&lt;h2 id=&#34;airflow-usage&#34;&gt;Airflow usage&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Which interface(s) of Airflow do you use as part of your current role?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface&lt;/td&gt;
&lt;td&gt;297&lt;/td&gt;
&lt;td&gt;96.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CLI&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;td&gt;40.91%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface, CLI&lt;/td&gt;
&lt;td&gt;117&lt;/td&gt;
&lt;td&gt;37.99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;API&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;19.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Original Airflow Graphical User Interface, CLI, API&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;10.39%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Custom (own created) Airflow Graphical User Interface&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;8.12%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It’s visible that usage of CLI goes in pair with using Airflow web UI. Our
survey included some UX related questions to allow us to understand how users
use Airflow webserver.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do you use the Graphical User Interface for?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do you use CLI for?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Airflow, which UI view(s) are important for you?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;plot3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here we see that the majority uses Web UI mostly for monitoring purposes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Monitoring DAGs&lt;/li&gt;
&lt;li&gt;Accessing logs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting result is that many people seem not to use backfilling as
there’s no other way than to do it by CLI.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What executor type do you use?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Celery&lt;/td&gt;
&lt;td&gt;138&lt;/td&gt;
&lt;td&gt;44.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Local&lt;/td&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;td&gt;27.60%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Kubernetes&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;16.88%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sequential&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;7.14%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;3.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The other option mostly consisted of information that someone uses a few types or is
migrating from one executor to another. What can be observed is an increase in usage
of Local and Kubernetes executors when compared to results from an earlier &lt;a href=&#34;https://ash.berlintaylor.com/writings/2019/02/airflow-user-survey-2019/&#34; target=&#34;_blank&#34;&gt;survey done
by Ash&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you use Kubernetes-based deployments for Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No - we do not plan to use Kubernetes near term&lt;/td&gt;
&lt;td&gt;88&lt;/td&gt;
&lt;td&gt;28.57%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes - setup on our own via Helm Chart or similar&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;21.10%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Not yet - but we use Kubernetes in our organization and we could move&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;19.81%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes - via managed service in the cloud (Composer / Astronomer etc.)&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;td&gt;14.61%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Not yet - but we plan to deploy Kubernetes in our organization soon&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;13.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The most interesting thing is that there’s nearly 30% of users who do not use Kubernetes,
and they are not going to move. This means we should keep other deployment options in
mind when working on Airflow 2.0. On the other hand, almost 70% of the users already
use Kubernetes, or it’s a viable option for them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you combine multiple DAGs?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;No, I don&amp;rsquo;t combine multiple DAGs&lt;/td&gt;
&lt;td&gt;127&lt;/td&gt;
&lt;td&gt;41.23%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes, through SubDAG&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;23.70%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Yes, by triggering another DAG&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;23.38%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Other&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;11.69%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the other category, 9 people explicitly mentioned using &lt;code&gt;ExternalTaskSensor&lt;/code&gt;,
and I think it could be treated as running subDAGs by triggering other DAGs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Do you use Airflow Plugins? If yes, what do you use it for?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Adding new operators/sensors and hooks&lt;/td&gt;
&lt;td&gt;187&lt;/td&gt;
&lt;td&gt;60.71%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I don&amp;rsquo;t use Airflow plugins&lt;/td&gt;
&lt;td&gt;109&lt;/td&gt;
&lt;td&gt;35.39%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding AppBuilder views &amp;amp; menu items&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;10.06%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding new executor&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;5.84%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Adding OperatorExtraLinks&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The high percentage - 60%  for “Adding new operators/sensors and hooks” is quite a
surprising result for some of us - especially that you do not actually need to use the
plugin mechanism to add any of those. Those are standard python objects, and you can
simply drop your hooks/operators/sensors code to &lt;code&gt;PYTHONPATH&lt;/code&gt; environment variable and
they will work. It seems that this may be a result of a lack of best practices guide.&lt;/p&gt;

&lt;p&gt;Plugins are more useful for adding views and menu items - yet only 10%.
OperatorExtraLinks are even more useful (though relatively new) feature, so it’s not
entirely surprising they are hardly used.&lt;/p&gt;

&lt;p&gt;It was also kind of surprising that someone at all uses plugins to use their own
executors. We considered removing that option recently - but now we have to rethink
our approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What metrics do you use to monitor Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There were a lot of different responses. Some use Prometheus and other services,
others do not use any monitoring. One of the interesting responses linked to this
solution for &lt;a href=&#34;https://github.com/mastak/airflow_operators_metrics&#34; target=&#34;_blank&#34;&gt;airflow_operators_metrics&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;external-services&#34;&gt;External services&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What external services do you use in your Airflow DAGs?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Amazon Web Services&lt;/td&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;51.95%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Internal company systems&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;48.7%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Hadoop / Spark / Flink / Other Apache software&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;38.64%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;36.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Microsoft Azure&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;9.09%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I do not use external services in my Airflow DAGs&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;5.84%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It’s not surprising that Amazon Web Services is leading the way as they are considered the most mature
cloud provider. Internal system and other Apache products on the next two positions are
quite understandable if we take into account that the majority uses Airflow for ETL processes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What external services do you use in your Airflow DAGs? (Mixed providers)&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs, Amazon Web Services&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;14.29%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Amazon Web Services, Microsoft Azure&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1.62%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Google Cloud Platform / Google APIs, Microsoft Azure&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This result is not surprising because companies usually prefer to stick with one cloud
provider.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do you integrate with external services?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Using Bash / Python operator&lt;/td&gt;
&lt;td&gt;220&lt;/td&gt;
&lt;td&gt;71.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Using existing, dedicated operators / hooks&lt;/td&gt;
&lt;td&gt;217&lt;/td&gt;
&lt;td&gt;70.45%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Using own, custom operators / hooks&lt;/td&gt;
&lt;td&gt;216&lt;/td&gt;
&lt;td&gt;70.13%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We had some anecdotal evidence that people use more Python/Bash operators than the
dedicated ones - but it looks like all ways of using Airflow to connect to external
services are equally popular.&lt;/p&gt;

&lt;h2 id=&#34;what-can-be-improved&#34;&gt;What can be improved&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;In your opinion, what could be improved in Airflow?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scheduler performance&lt;/td&gt;
&lt;td&gt;189&lt;/td&gt;
&lt;td&gt;61.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Web UI&lt;/td&gt;
&lt;td&gt;180&lt;/td&gt;
&lt;td&gt;58.44%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Logging, monitoring and alerting&lt;/td&gt;
&lt;td&gt;145&lt;/td&gt;
&lt;td&gt;47.08%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Examples, how-to, onboarding documentation&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;46.43%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Technical documentation&lt;/td&gt;
&lt;td&gt;137&lt;/td&gt;
&lt;td&gt;44.48%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Reliability&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;36.36%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;REST API&lt;/td&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;31.17%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Authentication and authorization&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;28.9%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;External integration e.g. AWS, GCP, Apache product&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;15.91%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CLI&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;13.31%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I don’t know&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1.62%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The results are rather quite self-explaining. Improved performance of Airflow, better
UI, and more telemetry are desirable. But this should go in pair with improved
documentation and resources about using the Airflow, especially when we
take into account the problem of onboarding new users.&lt;/p&gt;

&lt;p&gt;Another interesting point from that question is that only 16% think that operators
should be extended and improved. This suggests that we should focus on improving
Airflow core instead of adding more and more integrations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What would be the most interesting feature for you?&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No.&lt;/th&gt;
&lt;th&gt;%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Production-ready Airflow docker image&lt;/td&gt;
&lt;td&gt;175&lt;/td&gt;
&lt;td&gt;56.82%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Declarative way of writing DAGs / automated DAGs generation&lt;/td&gt;
&lt;td&gt;155&lt;/td&gt;
&lt;td&gt;50.32%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Horizontal Autoscaling&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;39.61%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Asynchronous Operators&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;31.49%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Stateless web server&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;26.3%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Knative Executor&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;15.58%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;I already have all I need&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;4.22%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Production Docker image wins, and it’s not a surprise. We all know that deploying
Airflow is not a plug and play process, and that’s why the official image is being
worked on by Jarek Potiuk. An unexpected result is that half of the users would like to
have a declarative way of creating DAGs. That seems to be something that is “against Airflow”
as we always emphasize the possibility of writing workflows in pure python. Stories
about DAG generators are not new and confirm that there’s a need for a way to
declare DAGs.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;If you think I missed something and you want to look for insights on your own the data is available
for you here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Original data: &lt;a href=&#34;https://storage.googleapis.com/airflow-survey/survey.csv&#34; target=&#34;_blank&#34;&gt;https://storage.googleapis.com/airflow-survey/survey.csv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Processed: &lt;a href=&#34;https://storage.googleapis.com/airflow-survey/airflow_survey_processed.csv&#34; target=&#34;_blank&#34;&gt;https://storage.googleapis.com/airflow-survey/airflow_survey_processed.csv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The processed data includes multi-choice options one-hot encoded. If you find any interesting
insight, please update the article (&lt;a href=&#34;https://github.com/apache/airflow-site/blob/master/CONTRIBUTE.md&#34; target=&#34;_blank&#34;&gt;make PR&lt;/a&gt;
to Airflow site).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: New Airflow website</title>
      <link>/blog/announcing-new-website/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/announcing-new-website/</guid>
      <description>
        
        
        &lt;p&gt;The brand &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;new Airflow website&lt;/a&gt; has arrived! Those who have been following the process know that the journey to update &lt;a href=&#34;https://airflow.readthedocs.io/en/1.10.6/&#34; target=&#34;_blank&#34;&gt;the old Airflow website&lt;/a&gt; started at the beginning of the year.
Thanks to sponsorship from the Cloud Composer team at Google that allowed us to
collaborate with &lt;a href=&#34;https://www.polidea.com/&#34; target=&#34;_blank&#34;&gt;Polidea&lt;/a&gt; and with their design studio &lt;a href=&#34;https://utilodesign.com/&#34; target=&#34;_blank&#34;&gt;Utilo&lt;/a&gt;, and deliver an awesome website.&lt;/p&gt;

&lt;p&gt;Documentation of open source projects is key to engaging new contributors in the maintenance,
development, and adoption of software. We want the Apache Airflow community to have
the best possible experience to contribute and use the project. We also took this opportunity to make the project
more accessible, and in doing so, increase its reach.&lt;/p&gt;

&lt;p&gt;In the past three and a half months, we have updated everything: created a more efficient landing page,
enhanced information architecture, and improved UX &amp;amp; UI. Most importantly, the website now has capabilities
to be translated into many languages. This is our effort to foster a more inclusive community around
Apache Airflow, and we look forward to seeing contributions in Spanish, Chinese, Russian, and other languages as well!&lt;/p&gt;

&lt;p&gt;We built our website on Docsy, a platform that is easy to use and contribute to. Follow
&lt;a href=&#34;https://github.com/apache/airflow-site/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;these steps&lt;/a&gt; to set up your environment and
to create your first pull request. You may also use
the new website for your own open source project as a template.
All of our &lt;a href=&#34;https://github.com/apache/airflow-site/tree/master&#34; target=&#34;_blank&#34;&gt;code is open and hosted on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Share your questions, comments, and suggestions with us, to help us improve the website.
We hope that this new design makes finding documentation about Airflow easier,
and that its improved accessibility increases adoption and use of Apache Airflow around the world.&lt;/p&gt;

&lt;p&gt;Happy browsing!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers</title>
      <link>/blog/apache-con-europe-2019-thoughts-and-insights-by-airflow-committers/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/apache-con-europe-2019-thoughts-and-insights-by-airflow-committers/</guid>
      <description>
        
        
        &lt;p&gt;Is it possible to create an organization that delivers tens of projects used by millions, nearly no one is paid for doing their job, and still, it has been fruitfully carrying on for more than 20 years? Apache Software Foundation proves it is possible. For the last two decades, ASF has been crafting a model called the Apache Way—a way of organizing and leading tech open source projects. Due to this approach, which is strongly based on the “community over code” motto, we can enjoy such awesome projects like Apache Spark, Flink, Beam, or Airflow (and many more).&lt;/p&gt;

&lt;p&gt;After this year’s ApacheCon, Polidea’s engineers talked with Committers of Apache projects, such as—Aizhamal Nurmamat kyzy, Felix Uellendall, and Fokko Driesprong—about insights to what makes the ASF such an amazing organization.&lt;/p&gt;

&lt;p&gt;You can read the &lt;a href=&#34;https://higrys.medium.com/apachecon-europe-2019-thoughts-and-insights-by-airflow-committers-9ff5f6938c99&#34; target=&#34;_blank&#34;&gt;insights after the ApacheCon 2019&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
